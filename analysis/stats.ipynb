{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "DATA_ROOT_DIR = \"../redmine_data\"\n",
    "DATA_DIR_AUTO = f\"{DATA_ROOT_DIR}/final/transcripts/auto\"\n",
    "DATA_DIR_MANUAL = f\"{DATA_ROOT_DIR}/final/transcripts/manual\"\n",
    "DATA_DIR_BEFORE_REVIEW = f\"{DATA_ROOT_DIR}/final/transcripts/before_review\"\n",
    "DATA_DIR_ASR_VERSIONS = f\"{DATA_ROOT_DIR}/final/asr_outputs\"\n",
    "PREFERENCE_ORDER = ['whisper', 'mixed', 'scratch']\n",
    "SPEAKER_TYPES = ['exam', 'cand']\n",
    "\n",
    "annotation2annot_duration = {}\n",
    "annotation2duration = {}\n",
    "\n",
    "def parse_xml_file(xml_file_path):\n",
    "\n",
    "    tree = ET.parse(xml_file_path)\n",
    "\n",
    "    root = tree.getroot()\n",
    "    recording = None\n",
    "    for r in root.iter('media'):\n",
    "        recording = r.attrib['url']\n",
    "        break\n",
    "    assert recording is not None\n",
    "\n",
    "    root = tree.getroot()\n",
    "    annot_duration = -1\n",
    "    for r in root.iter('annotDuration'):\n",
    "        annot_duration =  r.text\n",
    "        annot_duration = datetime.strptime(annot_duration, '%H:%M:%S').time()\n",
    "        annot_duration = annot_duration.hour * 3600 + annot_duration.minute * 60\n",
    "        break\n",
    "\n",
    "    #TODO: this is a foo value\n",
    "    duration = 0\n",
    "\n",
    "    utterances = []\n",
    "    last_end = -1\n",
    "    for u in root.iter('u'):\n",
    "        if u.text is not None and len(u.text) > 0:\n",
    "            #if last_end > float(u.attrib['start']):\n",
    "            #    print(f\"Overlapping utterances in {xml_file_path}\")\n",
    "            last_end = float(u.attrib['end'])\n",
    "            utterances.append({\n",
    "                'start': float(u.attrib['start']),\n",
    "                'end': float(u.attrib['end']),\n",
    "                'text': u.text,\n",
    "                'speaker': u.attrib['who'].lower(),\n",
    "                'source': u.attrib['source'] if 'source' in u.attrib else None\n",
    "            })\n",
    "            duration += float(u.attrib['end']) - float(u.attrib['start'])\n",
    "    annotation2duration[os.path.basename(xml_file_path)] = max(annotation2duration.get(recording, 0), duration)\n",
    "    annotation2annot_duration[os.path.basename(xml_file_path)] = max(annotation2annot_duration.get(recording, 0), annot_duration)\n",
    "    # print(xml_file_path, duration, annot_duration, len(utterances))\n",
    "    return recording, utterances\n",
    "\n",
    "\n",
    "def transcript_version_select(transcripts):\n",
    "    for p in PREFERENCE_ORDER:\n",
    "        for n, t in transcripts.items():\n",
    "            if p in n:\n",
    "                return t\n",
    "    return next(iter(transcripts.values()))\n",
    "\n",
    "def load_transcripts(data_dir):\n",
    "    recordings = {}\n",
    "\n",
    "    for xmlfile in os.listdir(data_dir):\n",
    "        if xmlfile.endswith('.xml'):\n",
    "            recording, utterances = parse_xml_file(os.path.join(data_dir, xmlfile))\n",
    "            annotations = recordings.get(recording, {})\n",
    "            annotations[xmlfile] = utterances\n",
    "            recordings[recording] = annotations\n",
    "    return recordings, {annot_name: annot for _, annotations in recordings.items() for annot_name, annot in annotations.items()}\n",
    "\n",
    "manual_recordings, manual_annotations = load_transcripts(DATA_DIR_MANUAL)\n",
    "auto_recordings, auto_annotations = load_transcripts(DATA_DIR_AUTO)\n",
    "before_review_recordings, before_review_annotations = load_transcripts(DATA_DIR_BEFORE_REVIEW)\n",
    "\n",
    "auto_recordings_versions = {}\n",
    "for ver_name in os.listdir(DATA_DIR_ASR_VERSIONS):\n",
    "    path = os.path.join(DATA_DIR_ASR_VERSIONS, ver_name)\n",
    "    if os.path.isdir(path):\n",
    "        recordings, annotations = load_transcripts(path)\n",
    "        auto_recordings_versions[ver_name] = annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "relevant = [\n",
    "    annotation2annot_duration[annot] for annot in manual_annotations.keys()\n",
    "]\n",
    "\n",
    "# Plotting the histogram\n",
    "plt.hist(relevant, bins=100, edgecolor='black')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram')\n",
    "\n",
    "# Displaying the histogram\n",
    "plt.show()\n",
    "\n",
    "# print annotations with duration more than 10000 seconds\n",
    "for annot, duration in annotation2annot_duration.items():\n",
    "    if duration > 10000:\n",
    "        print(annot, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "relevant = [\n",
    "    annotation2annot_duration[annot] / annotation2duration[annot] for annot in manual_annotations.keys()\n",
    "]\n",
    "\n",
    "# Plotting the histogram\n",
    "plt.hist(relevant, bins=100, edgecolor='black')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram')\n",
    "\n",
    "# Displaying the histogram\n",
    "plt.show()\n",
    "\n",
    "# print annotations with duration more than 10000 seconds\n",
    "for annot in manual_annotations.keys():\n",
    "    if annotation2annot_duration[annot] / annotation2duration[annot] > 30:\n",
    "        print(annot, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for t in PREFERENCE_ORDER:\n",
    "    relevant = [\n",
    "        annotation2annot_duration[annot] / annotation2duration[annot] for annot in manual_annotations.keys() if t in annot and 10000 > annotation2annot_duration[annot] > 0\n",
    "    ]\n",
    "    data.append([v for v in relevant if v < 1500])\n",
    "\n",
    "# plot three histograms in 3D plot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.view_init(elev=20, azim=250)\n",
    "for i, d in enumerate(reversed(data)):\n",
    "    hist, bins = np.histogram(d, bins=100)\n",
    "    xs = (bins[:-1] + bins[1:]) / 2\n",
    "    ax.bar(xs, hist, zs=i, zdir='y', alpha=0.8)\n",
    "\n",
    "fig.legend(list(reversed(PREFERENCE_ORDER)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import uttalign\n",
    "\n",
    "def sample_alignment():\n",
    "    #key = list(before_review_annotations.keys())[0]\n",
    "    key = \"A2ML_221205_07-ET-from_mixed.xml\"\n",
    "    print(key)\n",
    "    before_review_annotation = before_review_annotations[key]\n",
    "    auto_annotation = auto_annotations[key]\n",
    "    uttaligner = uttalign.MatrixAligner()\n",
    "\n",
    "    alignment = uttaligner.align_utterances(before_review_annotation, auto_annotation)\n",
    "    \n",
    "    aligned_texts = [(uttalign.extract_text(i, before_review_annotation), uttalign.extract_text(j, auto_annotation)) for i, j in alignment]\n",
    "    aligned_speakers = [(uttalign.extract_speaker(i, before_review_annotation), uttalign.extract_speaker(j, auto_annotation)) for i, j in alignment]\n",
    "\n",
    "    print(aligned_texts)\n",
    "    print(aligned_speakers)\n",
    "\n",
    "    #print_alignment(alignments, before_review_annotation, auto_annotation)\n",
    "\n",
    "sample_alignment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ1: Is manual post-editting of ASR outputs more eï¬€icient than manual transcription?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring in edit distance (Levenshtein, LCS). For all transcripts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from Bio.pairwise2 import format_alignment\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import uttalign\n",
    "import utils\n",
    "\n",
    "os.makedirs('alignments/rq1.1', exist_ok=True)\n",
    "\n",
    "#annot2texts = {k: concat_texts_by_speaker(v) for k, v in before_review_annotations.items()}\n",
    "before_review_auto_annot_pairs = {annot_name:(before_review_annotations[annot_name], auto_annotations[annot_name]) for annot_name in before_review_annotations}\n",
    "\n",
    "annot2edit_distances = {}\n",
    "annot2numwords = {}\n",
    "edit_distances = defaultdict(lambda: 0)\n",
    "totals = defaultdict(lambda: 0)\n",
    "\n",
    "aligner = uttalign.MatrixAligner(threshold_overlap=0.1)\n",
    "\n",
    "for annot_name, (before_review_annot, auto_annot) in before_review_auto_annot_pairs.items():\n",
    "    for transcript_type in PREFERENCE_ORDER:\n",
    "        if transcript_type in annot_name:\n",
    "            break\n",
    "\n",
    "    #src_speakers, src_texts = concat_texts_by_speaker(auto_annotations[annot_name])\n",
    "    #if 'scratch' in annot_name:\n",
    "    #    src_texts = ['' for _ in src_texts]\n",
    "\n",
    "    utt_alignment = aligner.align_utterances(before_review_annot, auto_annot)\n",
    "\n",
    "    aligned_texts = [(uttalign.extract_text(i, before_review_annot), uttalign.extract_text(j, auto_annot)) for i, j in utt_alignment]\n",
    "    aligned_speakers = [(uttalign.extract_speaker(i, before_review_annot), uttalign.extract_speaker(j, auto_annot)) for i, j in utt_alignment]\n",
    "\n",
    "    for (before_review_text, auto_text), (before_review_speaker, auto_speaker) in zip(aligned_texts, aligned_speakers):\n",
    "        if transcript_type == \"scratch\":\n",
    "            print(\"SCRATCH\")\n",
    "        for speaker_type in SPEAKER_TYPES:\n",
    "            if speaker_type.lower() in before_review_speaker.lower():\n",
    "                break\n",
    "        l = len(utils.normalize_text(before_review_text))\n",
    "        print(before_review_text, auto_text)\n",
    "        agree, disagree, word_alignment = uttalign.align_texts(before_review_text, auto_text)\n",
    "        if l and disagree / l > 0.9:\n",
    "            print(f\"{annot_name} {before_review_speaker} {transcript_type} {agree} {disagree}\")\n",
    "        with open(f'alignments/rq1.1/{annot_name}_{before_review_speaker}.txt', 'w') as f:\n",
    "            if word_alignment is not None:\n",
    "                f.write(format_alignment(*word_alignment))\n",
    "        edit_distances[f\"{transcript_type}:{speaker_type}\"] += disagree\n",
    "        edit_distances[transcript_type] += disagree\n",
    "        totals[f\"{transcript_type}:{speaker_type}\"] += l\n",
    "        totals[transcript_type] += l\n",
    "        \n",
    "        \n",
    "    #for speaker, text in texts.items():\n",
    "    #    l = len(normalize_text(text))\n",
    "    #    if speaker in src_speakers:\n",
    "    #   print(f\"{annot_name} {before_review_speaker} {transcript_type} {agree} {disagree}\")     agree, disagree, alignment = align_texts(text, src_texts[speaker] if speaker in src_speakers else '')\n",
    "    #        if disagree / l > 0.9:\n",
    "    #            print(f\"{annot_name} {speaker} {t} {agree} {disagree}\")\n",
    "    #        with open(f'alignments/rq1.1/{annot_name}_{speaker}.txt', 'w') as f:\n",
    "    #            f.write(format_alignment(*alignment))\n",
    "    #    else:\n",
    "    #        agree, disagree = 0, l\n",
    "    #    edit_distances[t] = edit_distances.get(t, 0) + disagree\n",
    "    #    totals[t] = totals.get(t, 0) + l\n",
    "    #    annot2edit_distances[annot_name] = annot2edit_distances.get(annot_name, 0) + disagree\n",
    "    #    annot2numwords[annot_name] = annot2numwords.get(annot_name, 0) + l\n",
    "\n",
    "\n",
    "print('Edit distances:', edit_distances)\n",
    "print('Total words:   ', totals)\n",
    "print('Edits per word:', [(k, v / totals[k]) for k, v in edit_distances.items()])\n",
    "\n",
    "#columns = ['whisper', 'mixed', 'scratch']\n",
    "columns = ['whisper', 'mixed']\n",
    "values = [totals[c] for c in columns]\n",
    "relativ = [edit_distances[c] / totals[c] for c in columns]\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(columns, relativ)\n",
    "ax.set_ylabel('Edit distance per word')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('alignments/rq1.1chars', exist_ok=True)\n",
    "\n",
    "annot2texts = {k: concat_texts_by_speaker(v) for k, v in before_review_annotations.items()}\n",
    "\n",
    "edit_distances = {}\n",
    "totals = {}\n",
    "\n",
    "for annot_name, (speakers, texts) in annot2texts.items():\n",
    "    for t in PREFERENCE_ORDER:\n",
    "        if t in annot_name:\n",
    "            break\n",
    "    src_speakers, src_texts = concat_texts_by_speaker(auto_annotations[annot_name])\n",
    "    if 'scratch' in annot_name:\n",
    "        src_texts = ['' for _ in src_texts]\n",
    "        \n",
    "    for speaker, text in texts.items():\n",
    "        l = len(normalize_text(text, char_level=True))\n",
    "        if speaker in src_speakers:\n",
    "            agree, disagree, alignment = align_texts(text, src_texts[speaker] if speaker in src_speakers else '')\n",
    "            if disagree / l > 0.9:\n",
    "                print(f\"{annot_name} {speaker} {t} {agree} {disagree}\")\n",
    "            with open(f'alignments/rq1.1chars/{annot_name}_{speaker}.txt', 'w') as f:\n",
    "                f.write(format_alignment(*alignment))\n",
    "        else:\n",
    "            agree, disagree = 0, l\n",
    "        edit_distances[t] = edit_distances.get(t, 0) + disagree\n",
    "        totals[t] = totals.get(t, 0) + l\n",
    "\n",
    "\n",
    "print('Edit distances:     ', edit_distances)\n",
    "print('Total characters:   ', totals)\n",
    "print('Edits per character:', [(k, v / totals[k]) for k, v in edit_distances.items()])\n",
    "\n",
    "columns = ['whisper', 'mixed', 'scratch']\n",
    "values = [totals[c] for c in columns]\n",
    "relativ = [edit_distances[c] / totals[c] for c in columns]\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(columns, relativ)\n",
    "ax.set_ylabel('Edit distance per character')\n",
    "IS_CHAR_LEVEL = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison over all recordings. Duration per word, per second (of the recording). Average edit distance (in words, chars)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('alignments/rq1.2', exist_ok=True)\n",
    "\n",
    "annot2texts = {k: concat_texts_by_speaker(v) for k, v in before_review_annotations.items()}\n",
    "\n",
    "durations = {}\n",
    "annotdurations = {}\n",
    "edits = {}\n",
    "words = {}\n",
    "\n",
    "for annot_name, (speakers, texts) in annot2texts.items():\n",
    "    for t in PREFERENCE_ORDER:\n",
    "        if t in annot_name:\n",
    "            break\n",
    "    if -1 < annotation2annot_duration[annot_name] < 10000: #TODO: this is \n",
    "        durations[t] = durations.get(t, 0) + annotation2duration[annot_name]\n",
    "        annotdurations[t] = annotdurations.get(t, 0) + annotation2annot_duration[annot_name]\n",
    "        edits[t] = edits.get(t, 0) + annot2edit_distances[annot_name]\n",
    "        words[t] = words.get(t, 0) + annot2numwords[annot_name]\n",
    "\n",
    "\n",
    "print('Total times: ', durations)\n",
    "print('Edit distances:', edits)\n",
    "print('Annot to recording ratio:', [(k, annotdurations[k] / v) for k, v in durations.items()])\n",
    "print('Annot time per word:', [(k, annotdurations[k] / v) for k, v in words.items()])\n",
    "print('Annot duration per edit:', [(k, annotdurations[k] / v) for k, v in edits.items()])\n",
    "print('Edit distance per word:', [(k, v / words[k]) for k, v in edits.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison over the recordings that were both manually transcribed and manually post-edited. Total duration. Total edit distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_recordings = [k for k, v in before_review_recordings.items() if any(['scratch' in a for a in v.keys()]) and len(v) > 1]\n",
    "\n",
    "total_words = {}\n",
    "total_duration = {}\n",
    "total_edits = {}\n",
    "total_annot_duration = {}\n",
    "\n",
    "for recording in relevant_recordings:\n",
    "    for annot_name in auto_recordings[recording]:\n",
    "        for t in PREFERENCE_ORDER:\n",
    "            if t in annot_name:\n",
    "                break\n",
    "        if t not in annot_name:\n",
    "            continue\n",
    "        if -1 < annotation2annot_duration[annot_name] < 10000: #TODO: this is \n",
    "            total_words[t] = total_words.get(t, 0) + annot2numwords[annot_name]\n",
    "            total_duration[t] = total_duration.get(t, 0) + annotation2duration[annot_name]\n",
    "            total_edits[t] = total_edits.get(t, 0) + annot2edit_distances[annot_name]\n",
    "            total_annot_duration[t] = total_annot_duration.get(t, 0) + annotation2annot_duration[annot_name]\n",
    "\n",
    "print('Relevant recordings:', len(relevant_recordings))\n",
    "print('Total words: ', total_words)\n",
    "print('Total duration:', total_duration)\n",
    "print('Total edits:', total_edits)\n",
    "print('Total annot duration:', total_annot_duration)\n",
    "print('Edits per word:', [(k, total_edits[k] / v) for k, v in total_words.items()])\n",
    "print('Duration per edit:', [(k, v / total_edits[k]) for k, v in total_duration.items()])\n",
    "print('Annot duration per edit:', [(k, v / total_edits[k]) for k, v in total_annot_duration.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the duration and edit distance correlate?\n",
    "TODO\n",
    "\n",
    "### Is there a significant difference across annotators? Some might prefer one way of transcribing over the other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot2numwords.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_annotator(annot_name):\n",
    "    n = annot_name.split('from')[0]\n",
    "    n = n[:-1].split('-')[-1]\n",
    "    assert len(n) > 0   \n",
    "    return n\n",
    "\n",
    "total_words = {}\n",
    "total_duration = {}\n",
    "total_edits = {}\n",
    "total_annot_duration = {}\n",
    "\n",
    "annotators = set([parse_annotator(a) for a in auto_annotations.keys()])\n",
    "\n",
    "\n",
    "\n",
    "for recording in manual_recordings.keys():\n",
    "    for annot_name in manual_recordings[recording]:\n",
    "        for t in PREFERENCE_ORDER:\n",
    "            if t in annot_name:\n",
    "                break\n",
    "        if t not in annot_name:\n",
    "            continue\n",
    "\n",
    "        if -1 < annotation2annot_duration[annot_name] < 10000: #TODO: this is \n",
    "            annotator = parse_annotator(annot_name)\n",
    "            total_words[f'{t}_{annotator}'] = total_words.get(f'{t}_{annotator}', 0) + annot2numwords[annot_name]\n",
    "            total_duration[f'{t}_{annotator}'] = total_duration.get(f'{t}_{annotator}', 0) + annotation2duration[annot_name]\n",
    "            total_edits[f'{t}_{annotator}'] = total_edits.get(f'{t}_{annotator}', 0) + annot2edit_distances[annot_name]\n",
    "            total_annot_duration[f'{t}_{annotator}'] = total_annot_duration.get(f'{t}_{annotator}', 0) + annotation2annot_duration[annot_name]\n",
    "\n",
    "print('Annotators:', annotators)\n",
    "# print('Total words: ', total_words)\n",
    "# print('Total duration:', total_duration)\n",
    "# print('Total edits:', total_edits)\n",
    "# print('Total annot duration:', total_annot_duration)\n",
    "print('Edits per word:', sorted([(k, total_edits[k] / v) for k, v in total_words.items()], key=lambda x: x[0]))\n",
    "print()\n",
    "print('Annot duration per word:', sorted([(k, total_annot_duration[k] / v) for k, v in total_words.items()], key=lambda x: x[0]))\n",
    "print()\n",
    "# print('Duration per edit:', [(k, v / total_edits[k]) for k, v in total_duration.items()])\n",
    "print('Annot duration per edit:', sorted([(k, v / total_edits[k]) for k, v in total_annot_duration.items()], key=lambda x: x[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns = ['whisper', 'mixed', 'scratch']\n",
    "rows = list(annotators)\n",
    "values = {k: total_annot_duration[k] / v for k, v in total_words.items()}\n",
    "# plot a heatmap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap([[values.get(f'{c}_{r}', -1) for c in columns] for r in rows], annot=True, ax=ax, xticklabels=columns, yticklabels=rows)\n",
    "ax.set_xlabel('Preference')\n",
    "ax.set_ylabel('Annotator')\n",
    "ax.set_title('Annotaton duration per word')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['whisper', 'mixed', 'scratch']\n",
    "rows = list(annotators)\n",
    "values = {k: total_annot_duration[k] / v for k, v in total_duration.items()}\n",
    "# plot a heatmap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap([[values.get(f'{c}_{r}', -1) for c in columns] for r in rows], annot=True, ax=ax, xticklabels=columns, yticklabels=rows)\n",
    "ax.set_xlabel('Preference')\n",
    "ax.set_ylabel('Annotator')\n",
    "ax.set_title('Annotator duration per recording duration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is there a significant difference across CEFR levels?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: same as above, but change the parsing of annotator name to level name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Is there a significant difference across exercise types?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Align AUTO to MANUAL on utterance level\n",
    "\n",
    "Idea: concat per speaker texts -> align AUTO to MANUAL -> resegment MANUAL based on the alignment and AUTO segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_texts_by_speaker_with_source(utterances):\n",
    "    texts = {}\n",
    "    for u in utterances:\n",
    "        words = texts.get(u['speaker'], [])\n",
    "        words.extend([\n",
    "            (w, u['source']) for w in normalize_text(u['text'])\n",
    "        ])\n",
    "        texts[u['speaker']] = words\n",
    "    return texts\n",
    "\n",
    "def match_fn(a, b):\n",
    "    a, b = set(a), set(b)\n",
    "    return len(a.intersection(b)) / len(a.union(b))\n",
    "\n",
    "relevant_annotation_names = [k for k, v in before_review_annotations.items() if 'mixed' in k]\n",
    "for annot_name in relevant_annotation_names[1:]:\n",
    "    before_review_texts = concat_texts_by_speaker_with_source(before_review_annotations[annot_name])\n",
    "    auto_texts = concat_texts_by_speaker_with_source(auto_annotations[annot_name])\n",
    "\n",
    "    for speaker in auto_texts.keys():\n",
    "        if speaker not in before_review_texts:\n",
    "            print(f\"Speaker {speaker} not found in before review texts for {annot_name}\")\n",
    "            continue\n",
    "        before_review_text = before_review_texts[speaker]\n",
    "        auto_text = auto_texts[speaker]\n",
    "        singletons = pairwise2.align.globalcs(auto_text, before_review_text, one_alignment_only=True, gap_char=[(None, None)], open=0, extend=0,\n",
    "                                                  match_fn=lambda a, b: 1 if a[0] == b[0] else 0)\n",
    "        print(f\"Alignment for {annot_name} {speaker} with score {singletons[0][2]}\")\n",
    "        curr_auto_text, curr_br_text, curr_source = [], [], None\n",
    "        for (auto_text, src), (br_text, _) in zip(singletons[0][0], singletons[0][1]):\n",
    "            if curr_source == None:\n",
    "                curr_source = src\n",
    "\n",
    "            if src != None and curr_source != src:\n",
    "                # if annot_name == 'A2ML_220919_01-KV-from_mixed.xml':\n",
    "                #     # print(f\"Current source: {curr_source}\")\n",
    "                #     # print(f\"Auto: {' '.join(curr_auto_text)}\")\n",
    "                #     # print(f\"BR:   {' '.join(curr_br_text)}\")\n",
    "                #     # print()\n",
    "                curr_auto_text, curr_br_text, curr_source = [], [], src\n",
    "\n",
    "            curr_auto_text += [auto_text] if auto_text != None else ''\n",
    "            curr_br_text += [br_text] if br_text != None else ''\n",
    "    #     break\n",
    "    # break\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ4: Is the human post-edited transcription biased towards the ASR system it was based on?\n",
    "## measure WER or other ASR metrics on outputs of multiple ASR systems while varying the reference transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_annot_type(filename):\n",
    "    return filename.split('from_')[-1].split('.')[0]\n",
    "\n",
    "def sort_by_time(utterances):\n",
    "    texts = []\n",
    "    for u in sorted(utterances, key=lambda x: x['start']):\n",
    "        texts.append(u['text'])\n",
    "    return ' '.join(texts)\n",
    "\n",
    "wers = {}\n",
    "for reference_asr, version in auto_recordings_versions.items():\n",
    "    agree, disagree = dict(), dict()\n",
    "    for recname, asr_transcript in version.items():\n",
    "        recname = recname.replace('.xml', '.mp3')\n",
    "        asr_transcript = sort_by_time(asr_transcript)\n",
    "        \n",
    "        if recname not in manual_recordings:\n",
    "            continue\n",
    "            \n",
    "        for annot_name, manual_transcript in manual_recordings[recname].items():\n",
    "            annot_type = parse_annot_type(annot_name)\n",
    "            manual_transcript = sort_by_time(manual_transcript)\n",
    "\n",
    "\n",
    "            if annot_type not in agree:\n",
    "                agree[annot_type] = 0\n",
    "                disagree[annot_type] = 0\n",
    "\n",
    "\n",
    "            a, d, _ = align_texts(asr_transcript, manual_transcript)\n",
    "            agree[annot_type] += a\n",
    "            disagree[annot_type] += d\n",
    "    print(f\"ASR version {reference_asr}:\")\n",
    "    for annot_type in agree.keys():\n",
    "        print(f\"  {annot_type}: {agree[annot_type]} / {disagree[annot_type]}; WER: {disagree[annot_type] / (agree[annot_type] + disagree[annot_type])}\")\n",
    "        wers[f'{reference_asr}_{annot_type}'] = disagree[annot_type] / (agree[annot_type] + disagree[annot_type])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a heatmap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(wers)\n",
    "fig, ax = plt.subplots()\n",
    "columns = set(c.split('_')[1] for c in wers.keys())\n",
    "rows = set(c.split('_')[0] for c in wers.keys())\n",
    "sns.heatmap([[wers.get(f'{r}_{c}', -1) for c in columns] for r in rows], annot=True, ax=ax, xticklabels=columns, yticklabels=rows)\n",
    "ax.set_xlabel('Preference')\n",
    "ax.set_ylabel('ASR version')\n",
    "ax.set_title('Word error rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto to Manual Utterances alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import uttalign\n",
    "\n",
    "os.makedirs('alignment_test', exist_ok=True)\n",
    "\n",
    "matrix_aligner = uttalign.MatrixAligner(threshold_overlap=0.05)\n",
    "bio_aligner = uttalign.BioAligner()\n",
    "\n",
    "for annot_file, manual_annotation in manual_annotations.items():\n",
    "    if annot_file != \"A2ML_221205_15-ZM-from_mixed.xml\":\n",
    "        continue\n",
    "    print(f\"Processing {annot_file}\")\n",
    "    auto_annotation = auto_annotations[annot_file]\n",
    "    if not len(auto_annotation):\n",
    "        continue\n",
    "    matrix_alignment = matrix_aligner.align_utterances(manual_annotation, auto_annotation)\n",
    "    with open(f'alignment_test/{annot_file}_matrix.txt', 'w') as f:\n",
    "        for a, b in matrix_alignment:\n",
    "            a_text = uttalign.extract_text(a, manual_annotation)\n",
    "            b_text = uttalign.extract_text(b, auto_annotation)\n",
    "            print(f\"MANUAL: {a_text}\", file=f)\n",
    "            print(f\"AUTO: {b_text}\", file=f)\n",
    "            print(file=f)\n",
    "\n",
    "    bio_alignment = bio_aligner.align_utterances(manual_annotation, auto_annotation)\n",
    "    with open(f'alignment_test/{annot_file}_bio.txt', 'w') as f:\n",
    "        for a, b in bio_alignment:\n",
    "            a_text = uttalign.extract_text(a, manual_annotation)\n",
    "            b_text = uttalign.extract_text(b, auto_annotation)\n",
    "            print(f\"MANUAL: {a_text}\", file=f)\n",
    "            print(f\"AUTO: {b_text}\", file=f)\n",
    "            print(file=f)\n",
    "\n",
    "\"\"\" with open('alignments.txt', 'w') as f:\n",
    "    for reference_asr, version in auto_recordings_versions.items():\n",
    "        for recname, asr_transcript in version.items():\n",
    "            recname = recname.replace('.xml', '.mp3')\n",
    "            \n",
    "            if recname not in manual_recordings:\n",
    "                continue\n",
    "                \n",
    "            f.write(f\"ASR version {reference_asr} for {recname}:\\n\")\n",
    "            for annot_name, manual_transcript in manual_recordings[recname].items():\n",
    "\n",
    "                alignment = align_utterances(manual_transcript,asr_transcript, )\n",
    "                for idx, (a, b) in enumerate(zip(alignment[0][0], alignment[0][1])):\n",
    "                    if b is not None:\n",
    "                        al = []\n",
    "                        if a is None:\n",
    "                            a = find_best_alignment_for_auto(alignment[0], idx)\n",
    "                        if a is not None:\n",
    "                            al.extend(a['original_utterances'])\n",
    "                        # print(b['original_utterances'])\n",
    "                        f.write(f\"{b['original_utterances']} -> {al}\\n\")\n",
    "        #         break\n",
    "        #     break\n",
    "        # break\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    " - Using WhisperX segmentation biases the number of utterances (e.g., 07-npi-test-B1.mp3 with 161 and 273)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: how to compute the total duration?\n",
    "total_duration = sum(list(recording.values())[0][-1]['end'] - list(recording.values())[0][0]['start'] for recording in recordings.values())\n",
    "print(f\"Total recordings:  {len(recordings)}\")\n",
    "print(f\"Total duration:    {total_duration:.02f} seconds ({total_duration/60:.02f} minutes)\")\n",
    "\n",
    "# TODO: should we take the max or select some specific version?\n",
    "total_utterances = sum(max(len(utterances) for utterances in transcripts.values()) for transcripts in recordings.values())\n",
    "print(f\"Total utterances:  {total_utterances}\")\n",
    "\n",
    "total_words = sum(\n",
    "    max(\n",
    "        sum(\n",
    "            len(utterance['text'].split())\n",
    "            for utterance in transcript\n",
    "        )\n",
    "        for transcript in recording.values()\n",
    "    )\n",
    " for recording in recordings.values())\n",
    "print(f\"Total words:       {total_words}\")\n",
    "\n",
    "\n",
    "num_versions_histogram = {}\n",
    "for recording, transcripts in recordings.items():\n",
    "    num_versions = len(transcripts)\n",
    "    num_versions_histogram[num_versions] = num_versions_histogram.get(num_versions, 0) + 1\n",
    "print(f\"Version histogram: {num_versions_histogram}\")\n",
    "\n",
    "num_speakers_histogram = {}\n",
    "for recording, transcripts in recordings.items():\n",
    "    num_speakers = max(len(set([u['speaker'] for u in utterances])) for utterances in transcripts.values())\n",
    "    num_speakers_histogram[num_speakers] = num_speakers_histogram.get(num_speakers, 0) + 1\n",
    "print(f\"Speaker histogram: {num_speakers_histogram}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Students Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "total_student_duration = sum(\n",
    "    max(\n",
    "        sum(\n",
    "            utterance['end'] - utterance['start']\n",
    "            for utterance in transcript\n",
    "            if utterance['speaker'] != 't'\n",
    "        )\n",
    "        for transcript in recording.values()\n",
    "    )\n",
    " for recording in recordings.values())\n",
    "print(f\"Total student duration:    {total_student_duration:.02f} seconds ({total_student_duration/60:.02f} minutes); {total_student_duration/total_duration*100:.02f}% of total duration\")\n",
    "\n",
    "total_student_utterances = sum(\n",
    "    max(\n",
    "        sum(\n",
    "            utterance['speaker'] != 't'\n",
    "            for utterance in transcript\n",
    "        )\n",
    "        for transcript in recording.values()\n",
    "    )\n",
    " for recording in recordings.values())\n",
    "print(f\"Total student utterances:  {total_student_utterances} ({total_student_utterances/total_utterances*100:.02f}% of total utterances)\")\n",
    "\n",
    "total_student_words = sum(\n",
    "    max(\n",
    "        sum(\n",
    "            len(utterance['text'].split())\n",
    "            for utterance in transcript\n",
    "            if utterance['speaker'] != 't'\n",
    "        )\n",
    "        for transcript in recording.values()\n",
    "    )\n",
    " for recording in recordings.values())\n",
    "print(f\"Total student words:       {total_student_words} ({total_student_words/total_words*100:.02f}% of total words)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for level in ['A1', 'A2', 'B1', 'B2', 'C1']:\n",
    "    relevant = [transcript_version_select(r) for n, r in recordings.items() if level in n]\n",
    "    \n",
    "    duration = sum(transcript[-1]['end'] - transcript[0]['start'] for transcript in relevant)\n",
    "    print(f'{level}: {len(relevant)} recordings; {duration:.02f} seconds ({duration/60:.02f} minutes)')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inter-annotator Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from mosestokenizer import MosesTokenizer, MosesPunctuationNormalizer\n",
    "\n",
    "if not os.path.exists('alignments'):\n",
    "    os.makedirs('alignments')\n",
    "\n",
    "punct_normalizer = MosesPunctuationNormalizer('cs')\n",
    "tokenizer = MosesTokenizer('cs')\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = text.lower().replace('\\n', \" \")\n",
    "    text = punct_normalizer(text)\n",
    "    text = tokenizer(text)\n",
    "    return [t for t in text if t.isalnum()]\n",
    "\n",
    "def align_texts_w(text1, text2):\n",
    "    text1 = normalize_text(text1)\n",
    "    text2 = normalize_text(text2)\n",
    "    alignments = pairwise2.align.globalxs(text1, text2, one_alignment_only=True, gap_char=['-'], open=-1, extend=-1)\n",
    "    \n",
    "    agree, disagree = 0, 0\n",
    "    for a, b in zip(alignments[0][0], alignments[0][1]):\n",
    "        if a == b:\n",
    "            agree += 1\n",
    "        else:\n",
    "            disagree += 1\n",
    "    return agree, disagree, alignments[0]\n",
    "\n",
    "def align_texts(text1, text2):\n",
    "    text1 = list(\"\".join(normalize_text(text1)))\n",
    "    text2 = list(\"\".join(normalize_text(text2)))\n",
    "    alignments = pairwise2.align.globalxs(text1, text2, one_alignment_only=True, open=-1, extend=-1, gap_char=['-'])\n",
    "    \n",
    "    agree, disagree = 0, 0\n",
    "    for a, b in zip(alignments[0][0], alignments[0][1]):\n",
    "        if a == b:\n",
    "            agree += 1\n",
    "        else:\n",
    "            disagree += 1\n",
    "    return agree, disagree, alignments[0]\n",
    "\n",
    "def concat_texts_by_speaker(utterances):\n",
    "    speakers = set([u['speaker'] for u in utterances])\n",
    "    texts = {}\n",
    "    for speaker in speakers:\n",
    "        texts[speaker] = ' '.join([u['text'] for u in utterances if u['speaker'] == speaker])\n",
    "    return speakers, texts\n",
    "\n",
    "relevant = {n: r for n, r in before_review_recordings.items() if len(r) > 1}\n",
    "\n",
    "def get_type(name):\n",
    "    if 'whisper' in name:\n",
    "        return 'whisper'\n",
    "    if 'mixed' in name:\n",
    "        return 'mixed'\n",
    "    if 'scratch' in name:\n",
    "        return 'scratch'\n",
    "    return 'unknown'\n",
    "\n",
    "agreements = {}\n",
    "disagreements = {}\n",
    "\n",
    "for name, transcripts in relevant.items():\n",
    "    print(f\"Recording {name}\")\n",
    "    for idx, (n1, t1) in enumerate(list(transcripts.items())):\n",
    "        speakers, t1 = concat_texts_by_speaker(t1)\n",
    "        for n2, t2 in list(transcripts.items())[idx+1:]:\n",
    "            speakers, t2 = concat_texts_by_speaker(t2)\n",
    "            for speaker in speakers:\n",
    "                if speaker not in t1 or speaker not in t2:\n",
    "                    continue\n",
    "                agree, disagree, alignment = align_texts(t1[speaker], t2[speaker])\n",
    "                \n",
    "                speaker = 'teacher' if speaker == 't' else 'student ' + speaker\n",
    "                print(f\"  {n1} vs {n2} ({speaker}):\\t {agree/(agree+disagree)*100:.02f}% agreement\\t {agree} agree, {disagree} disagree\")\n",
    "                if agree / (agree + disagree) < 0.25:\n",
    "                    continue\n",
    "                agreements[get_type(n1), get_type(n2)] = agreements.get((get_type(n1), get_type(n2)), 0) + agree\n",
    "                disagreements[get_type(n1), get_type(n2)] = disagreements.get((get_type(n1), get_type(n2)), 0) + disagree\n",
    "                \n",
    "                agreements[get_type(n2), get_type(n1)] = agreements[get_type(n1), get_type(n2)]\n",
    "                disagreements[get_type(n2), get_type(n1)] = disagreements[get_type(n1), get_type(n2)]\n",
    "\n",
    "                with open(f'alignments/{name}_{n1}_{n2}_{speaker}.txt', 'w') as f:\n",
    "                    f.write(format_alignment(*alignment))\n",
    "                    f.write(f\"\\n{agree/(agree+disagree)*100:.02f}% agreement\\t {agree} agree, {disagree} disagree\\n\")\n",
    "\n",
    "\n",
    "\n",
    "print(agreements)\n",
    "print(disagreements)\n",
    "print({k: v / (disagreements[k] + v) for k, v in agreements.items()})\n",
    "\n",
    "# plot confusion matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap([[agreements.get((r, c), 0) / (disagreements.get((r, c), 0) + agreements.get((r, c), 1)) for c in ['whisper', 'mixed', 'scratch']] for r in ['whisper', 'mixed', 'scratch']], annot=True, ax=ax, xticklabels=['whisper', 'mixed', 'scratch'], yticklabels=['whisper', 'mixed', 'scratch'])\n",
    "ax.set_xlabel('Reference')\n",
    "ax.set_ylabel('Hypothesis')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "naki",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
