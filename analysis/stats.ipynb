{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "DATA_DIR_AUTO = \"../../data/final/transcripts/auto\"\n",
    "DATA_DIR_MANUAL = \"../../data/final/transcripts/manual\"\n",
    "DATA_DIR_BEFORE_REVIEW = \"../../data/final/transcripts/before_review\"\n",
    "DATA_DIR_ASR_VERSIONS = \"../../data/final/asr_outputs\"\n",
    "PREFERENCE_ORDER = ['whisper', 'mixed', 'scratch']\n",
    "\n",
    "annotation2annot_duration = {}\n",
    "annotation2duration = {}\n",
    "\n",
    "def parse_xml_file(xml_file_path):\n",
    "\n",
    "    tree = ET.parse(xml_file_path)\n",
    "\n",
    "    root = tree.getroot()\n",
    "    recording = None\n",
    "    for r in root.iter('media'):\n",
    "        recording = r.attrib['url']\n",
    "        break\n",
    "    assert recording is not None\n",
    "\n",
    "    root = tree.getroot()\n",
    "    annot_duration = -1\n",
    "    for r in root.iter('annotDuration'):\n",
    "        annot_duration =  r.text\n",
    "        annot_duration = datetime.strptime(annot_duration, '%H:%M:%S').time()\n",
    "        annot_duration = annot_duration.hour * 3600 + annot_duration.minute * 60\n",
    "        break\n",
    "\n",
    "    #TODO: this is a foo value\n",
    "    duration = 0\n",
    "\n",
    "    utterances = []\n",
    "    last_end = -1\n",
    "    for u in root.iter('u'):\n",
    "        if u.text is not None and len(u.text) > 0:\n",
    "            # if  xml_file_path and last_end > float(u.attrib['start']):\n",
    "            #     print(f\"Overlapping utterances in {xml_file_path}\")\n",
    "            #     break\n",
    "            last_end = float(u.attrib['end'])\n",
    "            utterances.append({\n",
    "                'start': float(u.attrib['start']),\n",
    "                'end': float(u.attrib['end']),\n",
    "                'text': u.text,\n",
    "                'speaker': u.attrib['who'].lower(),\n",
    "                'source': u.attrib['source'] if 'source' in u.attrib else None\n",
    "            })\n",
    "            duration += float(u.attrib['end']) - float(u.attrib['start'])\n",
    "    annotation2duration[os.path.basename(xml_file_path)] = max(annotation2duration.get(recording, 0), duration)\n",
    "    annotation2annot_duration[os.path.basename(xml_file_path)] = max(annotation2annot_duration.get(recording, 0), annot_duration)\n",
    "    # print(xml_file_path, duration, annot_duration, len(utterances))\n",
    "    return recording, utterances\n",
    "\n",
    "\n",
    "def transcript_version_select(transcripts):\n",
    "    for p in PREFERENCE_ORDER:\n",
    "        for n, t in transcripts.items():\n",
    "            if p in n:\n",
    "                return t\n",
    "    return next(iter(transcripts.values()))\n",
    "\n",
    "def load_transcripts(data_dir):\n",
    "    recordings = {}\n",
    "\n",
    "    for xmlfile in os.listdir(data_dir):\n",
    "        if xmlfile.endswith('.xml'):\n",
    "            recording, utterances = parse_xml_file(os.path.join(data_dir, xmlfile))\n",
    "            annotations = recordings.get(recording, {})\n",
    "            annotations[xmlfile] = utterances\n",
    "            recordings[recording] = annotations\n",
    "    return recordings, {annot_name: annot for _, annotations in recordings.items() for annot_name, annot in annotations.items()}\n",
    "\n",
    "manual_recordings, manual_annotations = load_transcripts(DATA_DIR_MANUAL)\n",
    "auto_recordings, auto_annotations = load_transcripts(DATA_DIR_AUTO)\n",
    "before_review_recordings, before_review_annotations = load_transcripts(DATA_DIR_BEFORE_REVIEW)\n",
    "\n",
    "auto_recordings_versions = {}\n",
    "for ver_name in os.listdir(DATA_DIR_ASR_VERSIONS):\n",
    "    path = os.path.join(DATA_DIR_ASR_VERSIONS, ver_name)\n",
    "    if os.path.isdir(path):\n",
    "        recordings, annotations = load_transcripts(path)\n",
    "        auto_recordings_versions[ver_name] = annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "relevant = [\n",
    "    annotation2annot_duration[annot] for annot in manual_annotations.keys()\n",
    "]\n",
    "\n",
    "# Plotting the histogram\n",
    "plt.hist(relevant, bins=100, edgecolor='black')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram')\n",
    "\n",
    "# Displaying the histogram\n",
    "plt.show()\n",
    "\n",
    "# print annotations with duration more than 10000 minutes\n",
    "for annot, duration in annotation2annot_duration.items():\n",
    "    if duration > 10000:\n",
    "        print(annot, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "relevant = [\n",
    "    annotation2annot_duration[annot] / annotation2duration[annot] for annot in manual_annotations.keys()\n",
    "]\n",
    "\n",
    "# Plotting the histogram\n",
    "plt.hist(relevant, bins=100, edgecolor='black')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram')\n",
    "\n",
    "# Displaying the histogram\n",
    "plt.show()\n",
    "\n",
    "# print annotations with duration more than 10000 minutes\n",
    "for annot in manual_annotations.keys():\n",
    "    if annotation2annot_duration[annot] / annotation2duration[annot] > 30:\n",
    "        print(annot, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for t in PREFERENCE_ORDER:\n",
    "    relevant = [\n",
    "        annotation2annot_duration[annot] / annotation2duration[annot] for annot in manual_annotations.keys() if t in annot and 10000 > annotation2annot_duration[annot] > 0\n",
    "    ]\n",
    "    data.append([v for v in relevant if v < 1500])\n",
    "\n",
    "# plot three histograms in 3D plot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.view_init(elev=20, azim=250)\n",
    "for i, d in enumerate(reversed(data)):\n",
    "    hist, bins = np.histogram(d, bins=100)\n",
    "    xs = (bins[:-1] + bins[1:]) / 2\n",
    "    ax.bar(xs, hist, zs=i, zdir='y', alpha=0.8)\n",
    "\n",
    "fig.legend(list(reversed(PREFERENCE_ORDER)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from mosestokenizer import MosesTokenizer, MosesPunctuationNormalizer\n",
    "import re\n",
    "\n",
    "PUNCT_REPLACE = re.compile(r'[\\.,\\?!:]+')\n",
    "\n",
    "if not os.path.exists('alignments'):\n",
    "    os.makedirs('alignments')\n",
    "\n",
    "punct_normalizer = MosesPunctuationNormalizer('cs')\n",
    "tokenizer = MosesTokenizer('cs')\n",
    "\n",
    "excluded = set()\n",
    "\n",
    "IS_CHAR_LEVEL = False\n",
    "\n",
    "def normalize_text(text, char_level=False):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = punct_normalizer(text)\n",
    "    text = text.replace('-', '')\n",
    "    text = text.replace('*', '')\n",
    "    text = text.replace('\\'', '')\n",
    "    text = re.sub(PUNCT_REPLACE, ' ', text)\n",
    "    text = text.split()\n",
    "    if IS_CHAR_LEVEL or char_level:\n",
    "        return [t for t in ''.join(text).lower() if t.isalnum()]\n",
    "    return [t.lower() for t in text if t.isalnum()]\n",
    "\n",
    "def align_texts(text1, text2):\n",
    "    text1 = normalize_text(text1)\n",
    "    text2 = normalize_text(text2)\n",
    "    alignments = pairwise2.align.globalxs(text1, text2, one_alignment_only=True, gap_char=['-'], open=-1, extend=-1)\n",
    "    \n",
    "    agree, disagree = 0, 0\n",
    "    for a, b in zip(alignments[0][0], alignments[0][1]):\n",
    "        if a == b:\n",
    "            agree += 1\n",
    "        else:\n",
    "            disagree += 1\n",
    "    return agree, disagree, alignments[0]\n",
    "\n",
    "def concat_texts_by_speaker(utterances):\n",
    "    speakers = set([u['speaker'] for u in utterances])\n",
    "    texts = {}\n",
    "    for speaker in speakers:\n",
    "        texts[speaker] = ' '.join([u['text'] for u in utterances if u['speaker'] == speaker])\n",
    "    return speakers, texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ1: Is manual post-editting of ASR outputs more eï¬€icient than manual transcription?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring in edit distance (Levenshtein, LCS). For all transcripts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('alignments/rq1.1', exist_ok=True)\n",
    "\n",
    "annot2texts = {k: concat_texts_by_speaker(v) for k, v in before_review_annotations.items()}\n",
    "\n",
    "annot2edit_distances = {}\n",
    "annot2numwords = {}\n",
    "edit_distances = {}\n",
    "totals = {}\n",
    "\n",
    "for annot_name, (speakers, texts) in annot2texts.items():\n",
    "    for t in PREFERENCE_ORDER:\n",
    "        if t in annot_name:\n",
    "            break\n",
    "    src_speakers, src_texts = concat_texts_by_speaker(auto_annotations[annot_name])\n",
    "    if 'scratch' in annot_name:\n",
    "        src_texts = ['' for _ in src_texts]\n",
    "        \n",
    "    for speaker, text in texts.items():\n",
    "        l = len(normalize_text(text))\n",
    "        if speaker in src_speakers:\n",
    "            agree, disagree, alignment = align_texts(text, src_texts[speaker] if speaker in src_speakers else '')\n",
    "            if disagree / l > 0.9:\n",
    "                print(f\"{annot_name} {speaker} {t} {agree} {disagree}\")\n",
    "            with open(f'alignments/rq1.1/{annot_name}_{speaker}.txt', 'w') as f:\n",
    "                f.write(format_alignment(*alignment))\n",
    "        else:\n",
    "            agree, disagree = 0, l\n",
    "        edit_distances[t] = edit_distances.get(t, 0) + disagree\n",
    "        totals[t] = totals.get(t, 0) + l\n",
    "        annot2edit_distances[annot_name] = annot2edit_distances.get(annot_name, 0) + disagree\n",
    "        annot2numwords[annot_name] = annot2numwords.get(annot_name, 0) + l\n",
    "\n",
    "\n",
    "print('Edit distances:', edit_distances)\n",
    "print('Total words:   ', totals)\n",
    "print('Edits per word:', [(k, v / totals[k]) for k, v in edit_distances.items()])\n",
    "\n",
    "columns = ['whisper', 'mixed', 'scratch']\n",
    "values = [totals[c] for c in columns]\n",
    "relativ = [edit_distances[c] / totals[c] for c in columns]\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(columns, relativ)\n",
    "ax.set_ylabel('Edit distance per word')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_CHAR_LEVEL = True\n",
    "os.makedirs('alignments/rq1.1chars', exist_ok=True)\n",
    "\n",
    "annot2texts = {k: concat_texts_by_speaker(v) for k, v in before_review_annotations.items()}\n",
    "\n",
    "edit_distances = {}\n",
    "totals = {}\n",
    "\n",
    "for annot_name, (speakers, texts) in annot2texts.items():\n",
    "    for t in PREFERENCE_ORDER:\n",
    "        if t in annot_name:\n",
    "            break\n",
    "    src_speakers, src_texts = concat_texts_by_speaker(auto_annotations[annot_name])\n",
    "    if 'scratch' in annot_name:\n",
    "        src_texts = ['' for _ in src_texts]\n",
    "        \n",
    "    for speaker, text in texts.items():\n",
    "        l = len(normalize_text(text))\n",
    "        if speaker in src_speakers:\n",
    "            agree, disagree, alignment = align_texts(text, src_texts[speaker] if speaker in src_speakers else '')\n",
    "            if disagree / l > 0.9:\n",
    "                print(f\"{annot_name} {speaker} {t} {agree} {disagree}\")\n",
    "            with open(f'alignments/rq1.1chars/{annot_name}_{speaker}.txt', 'w') as f:\n",
    "                f.write(format_alignment(*alignment))\n",
    "        else:\n",
    "            agree, disagree = 0, l\n",
    "        edit_distances[t] = edit_distances.get(t, 0) + disagree\n",
    "        totals[t] = totals.get(t, 0) + l\n",
    "\n",
    "\n",
    "print('Edit distances:     ', edit_distances)\n",
    "print('Total characters:   ', totals)\n",
    "print('Edits per character:', [(k, v / totals[k]) for k, v in edit_distances.items()])\n",
    "\n",
    "columns = ['whisper', 'mixed', 'scratch']\n",
    "values = [totals[c] for c in columns]\n",
    "relativ = [edit_distances[c] / totals[c] for c in columns]\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(columns, relativ)\n",
    "ax.set_ylabel('Edit distance per character')\n",
    "IS_CHAR_LEVEL = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison over all recordings. Duration per word, per second (of the recording). Average edit distance (in words, chars)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('alignments/rq1.2', exist_ok=True)\n",
    "\n",
    "annot2texts = {k: concat_texts_by_speaker(v) for k, v in before_review_annotations.items()}\n",
    "\n",
    "durations = {}\n",
    "annotdurations = {}\n",
    "edits = {}\n",
    "words = {}\n",
    "\n",
    "for annot_name, (speakers, texts) in annot2texts.items():\n",
    "    for t in PREFERENCE_ORDER:\n",
    "        if t in annot_name:\n",
    "            break\n",
    "    if -1 < annotation2annot_duration[annot_name] < 10000: #TODO: this is \n",
    "        durations[t] = durations.get(t, 0) + annotation2duration[annot_name]\n",
    "        annotdurations[t] = annotdurations.get(t, 0) + annotation2annot_duration[annot_name]\n",
    "        edits[t] = edits.get(t, 0) + annot2edit_distances[annot_name]\n",
    "        words[t] = words.get(t, 0) + annot2numwords[annot_name]\n",
    "\n",
    "\n",
    "print('Total times: ', durations)\n",
    "print('Edit distances:', edits)\n",
    "print('Annot to recording ratio:', [(k, annotdurations[k] / v) for k, v in durations.items()])\n",
    "print('Annot time per word:', [(k, annotdurations[k] / v) for k, v in words.items()])\n",
    "print('Annot duration per edit:', [(k, annotdurations[k] / v) for k, v in edits.items()])\n",
    "print('Edit distance per word:', [(k, v / words[k]) for k, v in edits.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison over the recordings that were both manually transcribed and manually post-edited. Total duration. Total edit distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_recordings = [k for k, v in before_review_recordings.items() if any(['scratch' in a for a in v.keys()]) and len(v) > 1]\n",
    "\n",
    "total_words = {}\n",
    "total_duration = {}\n",
    "total_edits = {}\n",
    "total_annot_duration = {}\n",
    "\n",
    "for recording in relevant_recordings:\n",
    "    for annot_name in auto_recordings[recording]:\n",
    "        for t in PREFERENCE_ORDER:\n",
    "            if t in annot_name:\n",
    "                break\n",
    "        if t not in annot_name:\n",
    "            continue\n",
    "        if -1 < annotation2annot_duration[annot_name] < 10000: #TODO: this is \n",
    "            total_words[t] = total_words.get(t, 0) + annot2numwords[annot_name]\n",
    "            total_duration[t] = total_duration.get(t, 0) + annotation2duration[annot_name]\n",
    "            total_edits[t] = total_edits.get(t, 0) + annot2edit_distances[annot_name]\n",
    "            total_annot_duration[t] = total_annot_duration.get(t, 0) + annotation2annot_duration[annot_name]\n",
    "\n",
    "print('Relevant recordings:', len(relevant_recordings))\n",
    "print('Total words: ', total_words)\n",
    "print('Total duration:', total_duration)\n",
    "print('Total edits:', total_edits)\n",
    "print('Total annot duration:', total_annot_duration)\n",
    "print('Edits per word:', [(k, total_edits[k] / v) for k, v in total_words.items()])\n",
    "print('Duration per edit:', [(k, v / total_edits[k]) for k, v in total_duration.items()])\n",
    "print('Annot duration per edit:', [(k, v / total_edits[k]) for k, v in total_annot_duration.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the duration and edit distance correlate?\n",
    "TODO\n",
    "\n",
    "### Is there a significant difference across annotators? Some might prefer one way of transcribing over the other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_annotator(annot_name):\n",
    "    n = annot_name.split('from')[0]\n",
    "    n = n[:-1].split('-')[-1]\n",
    "    assert len(n) > 0   \n",
    "    return n\n",
    "\n",
    "total_words = {}\n",
    "total_duration = {}\n",
    "total_edits = {}\n",
    "total_annot_duration = {}\n",
    "\n",
    "annotators = set([parse_annotator(a) for a in auto_annotations.keys()])\n",
    "\n",
    "\n",
    "\n",
    "for recording in manual_recordings.keys():\n",
    "    for annot_name in manual_recordings[recording]:\n",
    "        for t in PREFERENCE_ORDER:\n",
    "            if t in annot_name:\n",
    "                break\n",
    "        if t not in annot_name:\n",
    "            continue\n",
    "\n",
    "        if -1 < annotation2annot_duration[annot_name] < 10000: #TODO: this is \n",
    "            annotator = parse_annotator(annot_name)\n",
    "            total_words[f'{t}_{annotator}'] = total_words.get(f'{t}_{annotator}', 0) + annot2numwords[annot_name]\n",
    "            total_duration[f'{t}_{annotator}'] = total_duration.get(f'{t}_{annotator}', 0) + annotation2duration[annot_name]\n",
    "            total_edits[f'{t}_{annotator}'] = total_edits.get(f'{t}_{annotator}', 0) + annot2edit_distances[annot_name]\n",
    "            total_annot_duration[f'{t}_{annotator}'] = total_annot_duration.get(f'{t}_{annotator}', 0) + annotation2annot_duration[annot_name]\n",
    "\n",
    "print('Annotators:', annotators)\n",
    "# print('Total words: ', total_words)\n",
    "# print('Total duration:', total_duration)\n",
    "# print('Total edits:', total_edits)\n",
    "# print('Total annot duration:', total_annot_duration)\n",
    "print('Edits per word:', sorted([(k, total_edits[k] / v) for k, v in total_words.items()], key=lambda x: x[0]))\n",
    "print()\n",
    "print('Annot duration per word:', sorted([(k, total_annot_duration[k] / v) for k, v in total_words.items()], key=lambda x: x[0]))\n",
    "print()\n",
    "# print('Duration per edit:', [(k, v / total_edits[k]) for k, v in total_duration.items()])\n",
    "print('Annot duration per edit:', sorted([(k, v / total_edits[k]) for k, v in total_annot_duration.items()], key=lambda x: x[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns = ['whisper', 'mixed', 'scratch']\n",
    "rows = list(annotators)\n",
    "values = {k: total_annot_duration[k] / v for k, v in total_words.items()}\n",
    "# plot a heatmap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap([[values.get(f'{c}_{r}', -1) for c in columns] for r in rows], annot=True, ax=ax, xticklabels=columns, yticklabels=rows)\n",
    "ax.set_xlabel('Preference')\n",
    "ax.set_ylabel('Annotator')\n",
    "ax.set_title('Annotaton duration per word')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['whisper', 'mixed', 'scratch']\n",
    "rows = list(annotators)\n",
    "values = {k: total_annot_duration[k] / v for k, v in total_duration.items()}\n",
    "# plot a heatmap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap([[values.get(f'{c}_{r}', -1) for c in columns] for r in rows], annot=True, ax=ax, xticklabels=columns, yticklabels=rows)\n",
    "ax.set_xlabel('Preference')\n",
    "ax.set_ylabel('Annotator')\n",
    "ax.set_title('Annotator duration per recording duration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is there a significant difference across CEFR levels?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: same as above, but change the parsing of annotator name to level name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Is there a significant difference across exercise types?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Align AUTO to MANUAL on utterance level\n",
    "\n",
    "Idea: concat per speaker texts -> align AUTO to MANUAL -> resegment MANUAL based on the alignment and AUTO segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_texts_by_speaker_with_source(utterances):\n",
    "    texts = {}\n",
    "    for u in utterances:\n",
    "        words = texts.get(u['speaker'], [])\n",
    "        words.extend([\n",
    "            (w, u['source']) for w in normalize_text(u['text'])\n",
    "        ])\n",
    "        texts[u['speaker']] = words\n",
    "    return texts\n",
    "\n",
    "def match_fn(a, b):\n",
    "    a, b = set(a), set(b)\n",
    "    return len(a.intersection(b)) / len(a.union(b))\n",
    "\n",
    "relevant_annotation_names = [k for k, v in before_review_annotations.items() if 'mixed' in k]\n",
    "for annot_name in relevant_annotation_names[1:]:\n",
    "    before_review_texts = concat_texts_by_speaker_with_source(before_review_annotations[annot_name])\n",
    "    auto_texts = concat_texts_by_speaker_with_source(auto_annotations[annot_name])\n",
    "\n",
    "    for speaker in auto_texts.keys():\n",
    "        if speaker not in before_review_texts:\n",
    "            print(f\"Speaker {speaker} not found in before review texts for {annot_name}\")\n",
    "            continue\n",
    "        before_review_text = before_review_texts[speaker]\n",
    "        auto_text = auto_texts[speaker]\n",
    "        singletons = pairwise2.align.globalcs(auto_text, before_review_text, one_alignment_only=True, gap_char=[(None, None)], open=0, extend=0,\n",
    "                                                  match_fn=lambda a, b: 1 if a[0] == b[0] else 0)\n",
    "        print(f\"Alignment for {annot_name} {speaker} with score {singletons[0][2]}\")\n",
    "        curr_auto_text, curr_br_text, curr_source = [], [], None\n",
    "        for (auto_text, src), (br_text, _) in zip(singletons[0][0], singletons[0][1]):\n",
    "            if curr_source == None:\n",
    "                curr_source = src\n",
    "\n",
    "            if src != None and curr_source != src:\n",
    "                # if annot_name == 'A2ML_220919_01-KV-from_mixed.xml':\n",
    "                #     # print(f\"Current source: {curr_source}\")\n",
    "                #     # print(f\"Auto: {' '.join(curr_auto_text)}\")\n",
    "                #     # print(f\"BR:   {' '.join(curr_br_text)}\")\n",
    "                #     # print()\n",
    "                curr_auto_text, curr_br_text, curr_source = [], [], src\n",
    "\n",
    "            curr_auto_text += [auto_text] if auto_text != None else ''\n",
    "            curr_br_text += [br_text] if br_text != None else ''\n",
    "    #     break\n",
    "    # break\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ4: Is the human post-edited transcription biased towards the ASR system it was based on?\n",
    "## measure WER or other ASR metrics on outputs of multiple ASR systems while varying the reference transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_annot_type(filename):\n",
    "    return filename.split('from_')[-1].split('.')[0]\n",
    "\n",
    "def sort_by_time(utterances):\n",
    "    texts = []\n",
    "    for u in sorted(utterances, key=lambda x: x['start']):\n",
    "        texts.append(u['text'])\n",
    "    return ' '.join(texts)\n",
    "\n",
    "wers = {}\n",
    "for reference_asr, version in auto_recordings_versions.items():\n",
    "    agree, disagree = dict(), dict()\n",
    "    for recname, asr_transcript in version.items():\n",
    "        recname = recname.replace('.xml', '.mp3')\n",
    "        asr_transcript = sort_by_time(asr_transcript)\n",
    "        \n",
    "        if recname not in manual_recordings:\n",
    "            continue\n",
    "            \n",
    "        for annot_name, manual_transcript in manual_recordings[recname].items():\n",
    "            annot_type = parse_annot_type(annot_name)\n",
    "            manual_transcript = sort_by_time(manual_transcript)\n",
    "\n",
    "\n",
    "            if annot_type not in agree:\n",
    "                agree[annot_type] = 0\n",
    "                disagree[annot_type] = 0\n",
    "\n",
    "\n",
    "            a, d, _ = align_texts(asr_transcript, manual_transcript)\n",
    "            agree[annot_type] += a\n",
    "            disagree[annot_type] += d\n",
    "    print(f\"ASR version {reference_asr}:\")\n",
    "    for annot_type in agree.keys():\n",
    "        print(f\"  {annot_type}: {agree[annot_type]} / {disagree[annot_type]}; WER: {disagree[annot_type] / (agree[annot_type] + disagree[annot_type])}\")\n",
    "        wers[f'{reference_asr}_{annot_type}'] = disagree[annot_type] / (agree[annot_type] + disagree[annot_type])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a heatmap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(wers)\n",
    "fig, ax = plt.subplots()\n",
    "columns = set(c.split('_')[1] for c in wers.keys())\n",
    "rows = set(c.split('_')[0] for c in wers.keys())\n",
    "sns.heatmap([[wers.get(f'{r}_{c}', -1) for c in columns] for r in rows], annot=True, ax=ax, xticklabels=columns, yticklabels=rows)\n",
    "ax.set_xlabel('Preference')\n",
    "ax.set_ylabel('ASR version')\n",
    "ax.set_title('Word error rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto to Manual Utterances alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import pairwise2\n",
    "def parse_annot_type(filename):\n",
    "    return filename.split('from_')[-1].split('.')[0]\n",
    "\n",
    "def sort_by_time(utterances):\n",
    "    return sorted(utterances, key=lambda x: x['start'])\n",
    "\n",
    "def concat_adjacent_speaker_utterances(utterances):\n",
    "    if len(utterances) == 0:\n",
    "        return []\n",
    "    new_utterances = [utterances[0].copy()]\n",
    "    new_utterances[-1]['original_utterances'] = [0]\n",
    "\n",
    "    for idx, u in enumerate(utterances[1:]):\n",
    "        if new_utterances[-1]['speaker'] == u['speaker']:\n",
    "            new_utterances[-1]['text'] += ' ' + u['text']\n",
    "            new_utterances[-1]['end'] = u['end']\n",
    "            new_utterances[-1]['original_utterances'].append(idx + 1)\n",
    "        else:\n",
    "            new_utterances.append(u.copy())\n",
    "            new_utterances[-1]['original_utterances'] = [idx + 1]\n",
    "    return new_utterances\n",
    "\n",
    "def time_overlap(a, b) -> float:\n",
    "    \"\"\"\n",
    "    Return the overlap of two time intervals\n",
    "    if the intervals do not overlap, return the distance between them\n",
    "    \"\"\"\n",
    "    if a['start'] > b['end'] or b['start'] > a['end']:\n",
    "        return min(a['end'], b['end']) - max(a['start'], b['start'])\n",
    "    return min(a['end'], b['end']) - max(a['start'], b['start'])\n",
    "\n",
    "def text_score(a, b):\n",
    "    a, b = normalize_text(a['text'], char_level=True), normalize_text(b['text'], char_level=True)\n",
    "    if len(a) == 0 or len(b) == 0:\n",
    "        return 0\n",
    "    alignment = pairwise2.align.globalms(a, b, one_alignment_only=True, gap_char=['-'], open=0, extend=0, match=1, mismatch=0)\n",
    "    return alignment[0].score\n",
    "\n",
    "def match_fn_with_threshold(a, b, threshold=0.1):\n",
    "    overlap = time_overlap(a, b)\n",
    "    if overlap < -threshold:\n",
    "        return float('-inf')\n",
    "    l = b['end'] - b['start']\n",
    "    return (overlap / l) * text_score(a, b) / max(1, len(normalize_text(b['text'], char_level=True))) \n",
    "\n",
    "def find_best_alignment_for_auto(alignment, idx):\n",
    "    candidates = []\n",
    "    for i in range(idx - 1, -1, -1):\n",
    "        if alignment[0][i] != None:\n",
    "            candidates.append(alignment[0][i])\n",
    "        '''\n",
    "        we need at least 2 utterances\n",
    "        since the manual transcript is concatenated\n",
    "        we know the 2 utterances have different speakers\n",
    "        '''\n",
    "        if len(candidates) > 1:  \n",
    "            break\n",
    "\n",
    "    for i in range(idx + 1, len(alignment[0])):\n",
    "        if alignment[0][i] != None:\n",
    "            candidates.append(alignment[0][i])\n",
    "        if len(candidates) > 3: # two utterances from the right side\n",
    "            break\n",
    "\n",
    "    match_fn = lambda a, b: match_fn_with_threshold(a, b, threshold=0.5) # be less strict\n",
    "    scores = [match_fn(c, alignment[1][idx]) for c in candidates]\n",
    "    \n",
    "    if len(scores) == 0 or max(scores) <= 0:\n",
    "        return None\n",
    "    return candidates[scores.index(max(scores))]\n",
    "\n",
    "def align_utterances(manual_utterances, asr_utterances):\n",
    "    manual_utterances = sort_by_time(manual_utterances)\n",
    "    manual_utterances = concat_adjacent_speaker_utterances(manual_utterances)\n",
    "\n",
    "    asr_utterances = sort_by_time(asr_utterances)\n",
    "    for idx, u in enumerate(asr_utterances):\n",
    "        asr_utterances[idx]['original_utterances'] = [idx]\n",
    "\n",
    "    match_fn = lambda a, b: match_fn_with_threshold(a, b, threshold=0.0) # be more strict\n",
    "    alignment = pairwise2.align.globalcx(manual_utterances, asr_utterances, one_alignment_only=True, gap_char=[None], match_fn=match_fn)\n",
    "    return alignment\n",
    "    \n",
    "\n",
    "wers = {}\n",
    "with open('alignments.txt', 'w') as f:\n",
    "    for reference_asr, version in auto_recordings_versions.items():\n",
    "        agree, disagree = dict(), dict()\n",
    "        for recname, asr_transcript in version.items():\n",
    "            recname = recname.replace('.xml', '.mp3')\n",
    "            \n",
    "            if recname not in manual_recordings:\n",
    "                continue\n",
    "                \n",
    "            f.write(f\"ASR version {reference_asr} for {recname}:\\n\")\n",
    "            for annot_name, manual_transcript in manual_recordings[recname].items():\n",
    "                annot_type = parse_annot_type(annot_name)\n",
    "\n",
    "                alignment = align_utterances(manual_transcript,asr_transcript, )\n",
    "                for idx, (a, b) in enumerate(zip(alignment[0][0], alignment[0][1])):\n",
    "                    if b is not None:\n",
    "                        al = []\n",
    "                        if a is None:\n",
    "                            a = find_best_alignment_for_auto(alignment[0], idx)\n",
    "                        if a is not None:\n",
    "                            al.extend(a['original_utterances'])\n",
    "                        # print(b['original_utterances'])\n",
    "                        f.write(f\"{b['original_utterances']} -> {al}\\n\")\n",
    "        #         break\n",
    "        #     break\n",
    "        # break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    " - Using WhisperX segmentation biases the number of utterances (e.g., 07-npi-test-B1.mp3 with 161 and 273)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: how to compute the total duration?\n",
    "total_duration = sum(list(recording.values())[0][-1]['end'] - list(recording.values())[0][0]['start'] for recording in recordings.values())\n",
    "print(f\"Total recordings:  {len(recordings)}\")\n",
    "print(f\"Total duration:    {total_duration:.02f} seconds ({total_duration/60:.02f} minutes)\")\n",
    "\n",
    "# TODO: should we take the max or select some specific version?\n",
    "total_utterances = sum(max(len(utterances) for utterances in transcripts.values()) for transcripts in recordings.values())\n",
    "print(f\"Total utterances:  {total_utterances}\")\n",
    "\n",
    "total_words = sum(\n",
    "    max(\n",
    "        sum(\n",
    "            len(utterance['text'].split())\n",
    "            for utterance in transcript\n",
    "        )\n",
    "        for transcript in recording.values()\n",
    "    )\n",
    " for recording in recordings.values())\n",
    "print(f\"Total words:       {total_words}\")\n",
    "\n",
    "\n",
    "num_versions_histogram = {}\n",
    "for recording, transcripts in recordings.items():\n",
    "    num_versions = len(transcripts)\n",
    "    num_versions_histogram[num_versions] = num_versions_histogram.get(num_versions, 0) + 1\n",
    "print(f\"Version histogram: {num_versions_histogram}\")\n",
    "\n",
    "num_speakers_histogram = {}\n",
    "for recording, transcripts in recordings.items():\n",
    "    num_speakers = max(len(set([u['speaker'] for u in utterances])) for utterances in transcripts.values())\n",
    "    num_speakers_histogram[num_speakers] = num_speakers_histogram.get(num_speakers, 0) + 1\n",
    "print(f\"Speaker histogram: {num_speakers_histogram}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Students Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "total_student_duration = sum(\n",
    "    max(\n",
    "        sum(\n",
    "            utterance['end'] - utterance['start']\n",
    "            for utterance in transcript\n",
    "            if utterance['speaker'] != 't'\n",
    "        )\n",
    "        for transcript in recording.values()\n",
    "    )\n",
    " for recording in recordings.values())\n",
    "print(f\"Total student duration:    {total_student_duration:.02f} seconds ({total_student_duration/60:.02f} minutes); {total_student_duration/total_duration*100:.02f}% of total duration\")\n",
    "\n",
    "total_student_utterances = sum(\n",
    "    max(\n",
    "        sum(\n",
    "            utterance['speaker'] != 't'\n",
    "            for utterance in transcript\n",
    "        )\n",
    "        for transcript in recording.values()\n",
    "    )\n",
    " for recording in recordings.values())\n",
    "print(f\"Total student utterances:  {total_student_utterances} ({total_student_utterances/total_utterances*100:.02f}% of total utterances)\")\n",
    "\n",
    "total_student_words = sum(\n",
    "    max(\n",
    "        sum(\n",
    "            len(utterance['text'].split())\n",
    "            for utterance in transcript\n",
    "            if utterance['speaker'] != 't'\n",
    "        )\n",
    "        for transcript in recording.values()\n",
    "    )\n",
    " for recording in recordings.values())\n",
    "print(f\"Total student words:       {total_student_words} ({total_student_words/total_words*100:.02f}% of total words)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for level in ['A1', 'A2', 'B1', 'B2', 'C1']:\n",
    "    relevant = [transcript_version_select(r) for n, r in recordings.items() if level in n]\n",
    "    \n",
    "    duration = sum(transcript[-1]['end'] - transcript[0]['start'] for transcript in relevant)\n",
    "    print(f'{level}: {len(relevant)} recordings; {duration:.02f} seconds ({duration/60:.02f} minutes)')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inter-annotator Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from mosestokenizer import MosesTokenizer, MosesPunctuationNormalizer\n",
    "\n",
    "if not os.path.exists('alignments'):\n",
    "    os.makedirs('alignments')\n",
    "\n",
    "punct_normalizer = MosesPunctuationNormalizer('cs')\n",
    "tokenizer = MosesTokenizer('cs')\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = text.lower().replace('\\n', \" \")\n",
    "    text = punct_normalizer(text)\n",
    "    text = tokenizer(text)\n",
    "    return [t for t in text if t.isalnum()]\n",
    "\n",
    "def align_texts_w(text1, text2):\n",
    "    text1 = normalize_text(text1)\n",
    "    text2 = normalize_text(text2)\n",
    "    alignments = pairwise2.align.globalxs(text1, text2, one_alignment_only=True, gap_char=['-'], open=-1, extend=-1)\n",
    "    \n",
    "    agree, disagree = 0, 0\n",
    "    for a, b in zip(alignments[0][0], alignments[0][1]):\n",
    "        if a == b:\n",
    "            agree += 1\n",
    "        else:\n",
    "            disagree += 1\n",
    "    return agree, disagree, alignments[0]\n",
    "\n",
    "def align_texts(text1, text2):\n",
    "    text1 = list(\"\".join(normalize_text(text1)))\n",
    "    text2 = list(\"\".join(normalize_text(text2)))\n",
    "    alignments = pairwise2.align.globalxs(text1, text2, one_alignment_only=True, open=-1, extend=-1, gap_char=['-'])\n",
    "    \n",
    "    agree, disagree = 0, 0\n",
    "    for a, b in zip(alignments[0][0], alignments[0][1]):\n",
    "        if a == b:\n",
    "            agree += 1\n",
    "        else:\n",
    "            disagree += 1\n",
    "    return agree, disagree, alignments[0]\n",
    "\n",
    "def concat_texts_by_speaker(utterances):\n",
    "    speakers = set([u['speaker'] for u in utterances])\n",
    "    texts = {}\n",
    "    for speaker in speakers:\n",
    "        texts[speaker] = ' '.join([u['text'] for u in utterances if u['speaker'] == speaker])\n",
    "    return speakers, texts\n",
    "\n",
    "relevant = {n: r for n, r in before_review_recordings.items() if len(r) > 1}\n",
    "\n",
    "def get_type(name):\n",
    "    if 'whisper' in name:\n",
    "        return 'whisper'\n",
    "    if 'mixed' in name:\n",
    "        return 'mixed'\n",
    "    if 'scratch' in name:\n",
    "        return 'scratch'\n",
    "    return 'unknown'\n",
    "\n",
    "agreements = {}\n",
    "disagreements = {}\n",
    "\n",
    "for name, transcripts in relevant.items():\n",
    "    print(f\"Recording {name}\")\n",
    "    for idx, (n1, t1) in enumerate(list(transcripts.items())):\n",
    "        speakers, t1 = concat_texts_by_speaker(t1)\n",
    "        for n2, t2 in list(transcripts.items())[idx+1:]:\n",
    "            speakers, t2 = concat_texts_by_speaker(t2)\n",
    "            for speaker in speakers:\n",
    "                if speaker not in t1 or speaker not in t2:\n",
    "                    continue\n",
    "                agree, disagree, alignment = align_texts(t1[speaker], t2[speaker])\n",
    "                \n",
    "                speaker = 'teacher' if speaker == 't' else 'student ' + speaker\n",
    "                print(f\"  {n1} vs {n2} ({speaker}):\\t {agree/(agree+disagree)*100:.02f}% agreement\\t {agree} agree, {disagree} disagree\")\n",
    "                if agree / (agree + disagree) < 0.25:\n",
    "                    continue\n",
    "                agreements[get_type(n1), get_type(n2)] = agreements.get((get_type(n1), get_type(n2)), 0) + agree\n",
    "                disagreements[get_type(n1), get_type(n2)] = disagreements.get((get_type(n1), get_type(n2)), 0) + disagree\n",
    "                \n",
    "                agreements[get_type(n2), get_type(n1)] = agreements[get_type(n1), get_type(n2)]\n",
    "                disagreements[get_type(n2), get_type(n1)] = disagreements[get_type(n1), get_type(n2)]\n",
    "\n",
    "                with open(f'alignments/{name}_{n1}_{n2}_{speaker}.txt', 'w') as f:\n",
    "                    f.write(format_alignment(*alignment))\n",
    "                    f.write(f\"\\n{agree/(agree+disagree)*100:.02f}% agreement\\t {agree} agree, {disagree} disagree\\n\")\n",
    "\n",
    "\n",
    "\n",
    "print(agreements)\n",
    "print(disagreements)\n",
    "print({k: v / (disagreements[k] + v) for k, v in agreements.items()})\n",
    "\n",
    "# plot confusion matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap([[agreements.get((r, c), 0) / (disagreements.get((r, c), 0) + agreements.get((r, c), 1)) for c in ['whisper', 'mixed', 'scratch']] for r in ['whisper', 'mixed', 'scratch']], annot=True, ax=ax, xticklabels=['whisper', 'mixed', 'scratch'], yticklabels=['whisper', 'mixed', 'scratch'])\n",
    "ax.set_xlabel('Reference')\n",
    "ax.set_ylabel('Hypothesis')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "naki",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
