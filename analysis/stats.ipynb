{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "DATA_ROOT_DIR = \"data\"\n",
    "DATA_DIR_AUTO = f\"{DATA_ROOT_DIR}/final/transcripts/auto\"\n",
    "DATA_DIR_MANUAL = f\"{DATA_ROOT_DIR}/final/transcripts/manual\"\n",
    "DATA_DIR_BEFORE_REVIEW = f\"{DATA_ROOT_DIR}/final/transcripts/before_review\"\n",
    "DATA_DIR_ASR_VERSIONS = f\"{DATA_ROOT_DIR}/final/asr_outputs\"\n",
    "PREFERENCE_ORDER = ['whisper', 'mixed', 'scratch']\n",
    "SPEAKER_TYPES = ['exam', 'cand']\n",
    "ANNOTATOR_NAMES = ['KR', 'MR', 'MH', 'AP', 'ET', 'KV', 'LR', 'ZM']\n",
    "\n",
    "annotation2annot_duration = {}\n",
    "annotation2duration = {}\n",
    "annotation2duration_per_speaker = defaultdict(dict)\n",
    "\n",
    "def parse_xml_file(xml_file_path, is_manual=False):\n",
    "\n",
    "    tree = ET.parse(xml_file_path)\n",
    "\n",
    "    root = tree.getroot()\n",
    "    recording = None\n",
    "    for r in root.iter('media'):\n",
    "        recording = r.attrib['url']\n",
    "        break\n",
    "    assert recording is not None\n",
    "\n",
    "    root = tree.getroot()\n",
    "    annot_duration = 0\n",
    "    for r in root.iter('annotDuration'):\n",
    "        annot_duration += float(r.text)\n",
    "\n",
    "    #TODO: this is a foo value\n",
    "    duration = 0\n",
    "    duration_per_speaker = defaultdict(int)\n",
    "\n",
    "    utterances = []\n",
    "    last_end = -1\n",
    "    for u in root.iter('u'):\n",
    "        if u.text is not None and len(u.text) > 0:\n",
    "            #if last_end > float(u.attrib['start']):\n",
    "            #    print(f\"Overlapping utterances in {xml_file_path}\")\n",
    "            last_end = float(u.attrib['end'])\n",
    "            utterances.append({\n",
    "                'start': float(u.attrib['start']),\n",
    "                'end': float(u.attrib['end']),\n",
    "                'text': u.text,\n",
    "                'speaker': u.attrib['who'].lower(),\n",
    "                'source': u.attrib['source'] if 'source' in u.attrib else None\n",
    "            })\n",
    "            duration += float(u.attrib['end']) - float(u.attrib['start'])\n",
    "            duration_per_speaker[u.attrib['who'].lower()] += float(u.attrib['end']) - float(u.attrib['start'])\n",
    "    transcript_basename = os.path.basename(xml_file_path)\n",
    "    annotation2duration[transcript_basename] = max(annotation2duration.get(recording, 0), duration)\n",
    "    annotation2duration[recording] = max(annotation2duration.get(recording, 0), duration)\n",
    "    if is_manual:\n",
    "        annotation2duration_per_speaker[transcript_basename] = {k: max(annotation2duration_per_speaker[transcript_basename].get(k, 0), v) for k, v in duration_per_speaker.items()}\n",
    "        annotation2duration_per_speaker[recording] = {k: max(annotation2duration_per_speaker[recording].get(k, 0), v) for k, v in duration_per_speaker.items()}\n",
    "    # print(xml_file_path, duration, annot_duration, len(utterances))\n",
    "    return recording, utterances, annot_duration\n",
    "\n",
    "\n",
    "def transcript_version_select(transcripts):\n",
    "    for p in PREFERENCE_ORDER:\n",
    "        for n, t in transcripts.items():\n",
    "            if p in n:\n",
    "                return t\n",
    "    return next(iter(transcripts.values()))\n",
    "\n",
    "def load_transcripts(data_dir, is_manual=False):\n",
    "    recordings = {}\n",
    "    annot_durations = {}\n",
    "\n",
    "    for xmlfile in os.listdir(data_dir):\n",
    "        if not xmlfile.endswith('.xml'):\n",
    "            continue\n",
    "        # skip all recordings that are not pure A2 level\n",
    "        if 'A2' not in xmlfile or 'bA2' in xmlfile or 'uA2' in xmlfile:\n",
    "            continue\n",
    "        # skip transcripts annotated by KR and MR: they lack the transcript of the examiner\n",
    "        if 'KR' in xmlfile or 'MR' in xmlfile:\n",
    "            continue\n",
    "        recording, utterances, annot_duration = parse_xml_file(os.path.join(data_dir, xmlfile), is_manual)\n",
    "        annotations = recordings.get(recording, {})\n",
    "        annotations[xmlfile] = utterances\n",
    "        annot_durations[xmlfile] = annot_duration\n",
    "        recordings[recording] = annotations\n",
    "    return recordings, {annot_name: annot for _, annotations in recordings.items() for annot_name, annot in annotations.items()}, annot_durations\n",
    "\n",
    "manual_recordings, manual_annotations, manual_annot_durations = load_transcripts(DATA_DIR_MANUAL, is_manual=True)\n",
    "auto_recordings, auto_annotations, _ = load_transcripts(DATA_DIR_AUTO)\n",
    "before_review_recordings, before_review_annotations, before_review_annot_durations = load_transcripts(DATA_DIR_BEFORE_REVIEW, is_manual=True)\n",
    "\n",
    "auto_recordings_versions = {}\n",
    "for ver_name in os.listdir(DATA_DIR_ASR_VERSIONS):\n",
    "    path = os.path.join(DATA_DIR_ASR_VERSIONS, ver_name)\n",
    "    if os.path.isdir(path):\n",
    "        recordings, annotations, _ = load_transcripts(path)\n",
    "        auto_recordings_versions[ver_name] = annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Apply the seaborn theme\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "os.makedirs('figures', exist_ok=True)\n",
    "\n",
    "forstat_recordings = before_review_recordings\n",
    "#forstat_recordings = manual_recordings\n",
    "forstat_annotations = before_review_annotations\n",
    "#forstat_annotations = manual_annotations\n",
    "\n",
    "print(f\"Number of recordings: {len(forstat_recordings)}\")\n",
    "print(f\"Total length of recordings: {sum([annotation2duration[rec] for rec in forstat_recordings])/60} minutes\")\n",
    "print(f\"Avg length of recording: {sum([annotation2duration[rec] for rec in forstat_recordings]) / len(forstat_recordings)} seconds\")\n",
    "recording_lengths_per_speaker = defaultdict(int)\n",
    "for speaker_type in SPEAKER_TYPES:\n",
    "    length_per_speaker_type = sum([annotation2duration_per_speaker[rec][speaker] for rec in forstat_recordings for speaker in annotation2duration_per_speaker[rec] if speaker.startswith(speaker_type)])\n",
    "    print(f\"Total length of recordings for {speaker_type}: {length_per_speaker_type/60} minutes\")\n",
    "\n",
    "print(f\"Number of transcripts: {len(forstat_annotations)}\")\n",
    "print(f\"Number of recording with more than a one transcript: {len([rec for rec, annots in forstat_recordings.items() if len(annots) > 1])}\")\n",
    "\n",
    "# histogram of the number of transcripts per recordin\n",
    "for recname, annots in forstat_recordings.items():\n",
    "    if len(annots) > 1:\n",
    "        print(recname, len(annots))\n",
    "transcripts_per_recording = [len(annots) for annots in forstat_recordings.values()]\n",
    "\n",
    "# Calculate bin edges and midpoints\n",
    "bin_edges = range(1, max(transcripts_per_recording) + 2)\n",
    "bin_midpoints = [(edge + bin_edges[i+1])/2 for i, edge in enumerate(bin_edges[:-1])]\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(9, 4))  # Set the figure size to adjust the height and width\n",
    "#plt.hist(transcripts_per_recording, bins=bin_edges)\n",
    "sns.histplot(transcripts_per_recording, bins=bin_edges, kde=False)\n",
    "plt.xticks(bin_midpoints, labels=[str(int(x)) for x in bin_midpoints])  # Set the x-ticks to the midpoints, converting them to string if needed\n",
    "#plt.title('Number of transcripts per recording')\n",
    "plt.xlabel('Number of transcripts')\n",
    "plt.ylabel('Number of recordings')\n",
    "plt.savefig('figures/transcripts_per_recording.pdf', format='pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "stats_annot_durations = before_review_annot_durations\n",
    "\n",
    "relevant = [\n",
    "    duration for annot, duration in stats_annot_durations.items()\n",
    "]\n",
    "\n",
    "print(f\"Number of transcripts for which we measured duration: {len([duration for duration in relevant if duration > 0])}\")\n",
    "\n",
    "# Plotting the histogram\n",
    "plt.hist(relevant, bins=100)\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram')\n",
    "\n",
    "# Displaying the histogram\n",
    "plt.show()\n",
    "\n",
    "# print annotations with duration more than 10000 seconds\n",
    "for annot, duration in stats_annot_durations.items():\n",
    "    if duration > 10000:\n",
    "        print(annot, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "stats_annot_durations = before_review_annot_durations\n",
    "\n",
    "relevant = [\n",
    "    duration / annotation2duration[annot] for annot, duration in stats_annot_durations.items()\n",
    "]\n",
    "\n",
    "# Plotting the histogram\n",
    "plt.hist(relevant, bins=100)\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram')\n",
    "\n",
    "# Displaying the histogram\n",
    "plt.show()\n",
    "\n",
    "# print annotations with duration more than 10000 seconds\n",
    "for annot, duration in stats_annot_durations.items():\n",
    "    if duration / annotation2duration[annot] > 30:\n",
    "        print(annot, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_annot_durations = before_review_annot_durations\n",
    "\n",
    "data = []\n",
    "for t in PREFERENCE_ORDER:\n",
    "    relevant = [\n",
    "        # The sessions where TEITOK timed out have been already filtered out. No need of filtering out durations longer than 10000 seconds.\n",
    "        #duration / annotation2duration[annot] for annot, duration in stats_annot_durations.items() if t in annot and 10000 > duration > 0\n",
    "        duration / annotation2duration[annot] for annot, duration in stats_annot_durations.items() if t in annot and duration > 0\n",
    "    ]\n",
    "    data.append([v for v in relevant])\n",
    "\n",
    "# plot three histograms in 3D plot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.view_init(elev=20, azim=250)\n",
    "for i, d in enumerate(reversed(data)):\n",
    "    hist, bins = np.histogram(d, bins=100)\n",
    "    xs = (bins[:-1] + bins[1:]) / 2\n",
    "    ax.bar(xs, hist, zs=i, zdir='y', alpha=0.8)\n",
    "\n",
    "fig.legend(list(reversed(PREFERENCE_ORDER)))\n",
    "\n",
    "for i, t in enumerate(PREFERENCE_ORDER):\n",
    "    relevant = data[i]\n",
    "    print(f\"Average duration for {t}: {sum(relevant)/len(relevant)}\")\n",
    "    print(f\"Max duration for {t}: {max(relevant)}\")\n",
    "    print(f\"Min duration for {t}: {min(relevant)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import uttalign\n",
    "\n",
    "def sample_alignment():\n",
    "    #key = list(before_review_annotations.keys())[0]\n",
    "    key = \"A2ML_221205_07-ET-from_mixed.xml\"\n",
    "    print(key)\n",
    "    before_review_annotation = before_review_annotations[key]\n",
    "    auto_annotation = auto_annotations[key]\n",
    "    #uttaligner = uttalign.MatrixAligner()\n",
    "    uttaligner = uttalign.BioAligner()\n",
    "\n",
    "    alignment = uttaligner.align_utterances(before_review_annotation, auto_annotation)\n",
    "\n",
    "    print(alignment)\n",
    "    \n",
    "    aligned_texts = [(uttalign.extract_text(i, before_review_annotation), uttalign.extract_text(j, auto_annotation)) for i, j in alignment]\n",
    "    aligned_speakers = [(uttalign.extract_speaker(i, before_review_annotation), uttalign.extract_speaker(j, auto_annotation)) for i, j in alignment]\n",
    "\n",
    "    print(aligned_texts)\n",
    "    print(aligned_speakers)\n",
    "\n",
    "    #print_alignment(alignments, before_review_annotation, auto_annotation)\n",
    "\n",
    "sample_alignment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ1: Is manual post-editting of ASR outputs more eﬀicient than manual transcription?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take only the transcripts where the annotation duration was logged\n",
    "stats_annot_durations = {annot: duration for annot, duration in before_review_annot_durations.items() if duration > 0}\n",
    "stats_annotations = before_review_annotations\n",
    "print(len(stats_annot_durations.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# real-time factor for each annotator\n",
    "rtf_per_user = defaultdict(list)\n",
    "\n",
    "for annot_name, duration in stats_annot_durations.items():\n",
    "    rtf = duration / annotation2duration[annot_name]\n",
    "    user = annot_name.split('-')[1]\n",
    "    rtf_per_user[user].append(rtf)\n",
    "\n",
    "#color = plt.rainbow(np.linspace(0, 1, len(overall_times_per_user.keys())))\n",
    "\n",
    "print(list(rtf_per_user.keys()))\n",
    "print([np.mean(times) for times in rtf_per_user.values()])\n",
    "\n",
    "\n",
    "legend_labels = [f\"Annotator {i}\" for i in range(1, len(rtf_per_user.keys())+1)]\n",
    "fig, ax = plt.subplots()\n",
    "df = pd.DataFrame({user: pd.Series(times) for user, times in rtf_per_user.items()})\n",
    "df.plot(kind='density', ax=ax)\n",
    "ax.legend(legend_labels)\n",
    "ax.set_xlabel('Real-time factor')\n",
    "#ax.set_xlim(0, 50)\n",
    "# set the figure size\n",
    "fig.set_size_inches(7, 4)\n",
    "#plt.title('Histogram of overall times per user')\n",
    "fig.savefig('figures/annot-speed-per-user.pdf', format=\"pdf\")\n",
    "#plt.ylabel('Frequency')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from Bio.pairwise2 import format_alignment\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import uttalign\n",
    "import utils\n",
    "\n",
    "def calculate_edit_distances(to_annotations, from_annotations, align_directory=None):\n",
    "\n",
    "    if align_directory is not None:\n",
    "        os.makedirs(align_directory, exist_ok=True)\n",
    "\n",
    "    to_from_annot_pairs = {annot_name:(to_annotations[annot_name], from_annotations[annot_name]) for annot_name in to_annotations}\n",
    "\n",
    "    aligner = uttalign.BioAligner()\n",
    "    # aligner = uttalign.MatrixAligner(threshold_overlap=0.1)\n",
    "\n",
    "    edit_counts = {speaker_type: defaultdict(list) for speaker_type in SPEAKER_TYPES + ['all']} \n",
    "    to_lengths = {speaker_type: defaultdict(list) for speaker_type in SPEAKER_TYPES + ['all']}\n",
    "    ceds = {speaker_type: defaultdict(list) for speaker_type in SPEAKER_TYPES + ['all']}\n",
    "\n",
    "    for annot_name, (to_annot, from_annot) in to_from_annot_pairs.items():\n",
    "        # create empty file for alignment\n",
    "        if align_directory is not None:\n",
    "            with open(f'{align_directory}/{annot_name}.txt', 'w') as f:\n",
    "                f.write(\"\")\n",
    "        \n",
    "        for transcript_type in PREFERENCE_ORDER:\n",
    "            if transcript_type in annot_name:\n",
    "                break\n",
    "        \n",
    "        utt_alignment = aligner.align_utterances(to_annot, from_annot)\n",
    "\n",
    "        aligned_texts = [(uttalign.extract_text(i, to_annot), uttalign.extract_text(j, from_annot)) for i, j in utt_alignment]\n",
    "        aligned_speakers = [(uttalign.extract_speaker(i, to_annot), uttalign.extract_speaker(j, from_annot)) for i, j in utt_alignment]\n",
    "\n",
    "        edit_count = {speaker_type: 0 for speaker_type in SPEAKER_TYPES + ['all']}\n",
    "        to_length = {speaker_type: 0 for speaker_type in SPEAKER_TYPES + ['all']}\n",
    "\n",
    "        for (to_text, from_text), (to_speaker, from_speaker) in zip(aligned_texts, aligned_speakers):\n",
    "            speaker_type = to_speaker\n",
    "            if to_speaker is not None:\n",
    "                for speaker_type in SPEAKER_TYPES:\n",
    "                    if speaker_type.lower() in to_speaker.lower():\n",
    "                        break\n",
    "            l = len(utils.normalize_text(to_text, char_level=True))\n",
    "            #l = len([c for c in before_review_text])\n",
    "            #print(before_review_text, auto_text)\n",
    "            agree, disagree, word_alignment = uttalign.align_texts(to_text, from_text, char_level=True)\n",
    "            # if l and disagree / l > 0.9:\n",
    "            #     print(f\"{annot_name} {before_review_speaker} {transcript_type} {agree} {disagree}\")\n",
    "            # append alignment to file\n",
    "            if align_directory is not None:\n",
    "                with open(f'{align_directory}/{annot_name}.txt', 'a') as f:\n",
    "                    f.write(f\"SPEAKER_TYPE: {speaker_type}\\n\")\n",
    "                    if word_alignment is not None:\n",
    "                        f.write(format_alignment(*word_alignment))\n",
    "                    else:\n",
    "                        f.write(f\"{to_text}\\n------------{from_text}\\n\")\n",
    "            if speaker_type:\n",
    "                edit_count[speaker_type] += disagree\n",
    "                to_length[speaker_type] += l\n",
    "            edit_count['all'] += disagree\n",
    "            to_length['all'] += l\n",
    "        \n",
    "        for speaker_type in SPEAKER_TYPES + ['all']:\n",
    "            edit_counts[speaker_type][transcript_type].append(edit_count[speaker_type])\n",
    "            to_lengths[speaker_type][transcript_type].append(to_length[speaker_type])\n",
    "            ceds[speaker_type][transcript_type].append(edit_count[speaker_type] / to_length[speaker_type] if to_length[speaker_type] > 0 else 0)\n",
    "            edit_counts[speaker_type]['all'].append(edit_count[speaker_type])\n",
    "            to_lengths[speaker_type]['all'].append(to_length[speaker_type])\n",
    "            ceds[speaker_type]['all'].append(edit_count[speaker_type] / to_length[speaker_type] if to_length[speaker_type] > 0 else 0)\n",
    "    return ceds, edit_counts, to_lengths\n",
    "\n",
    "ceds, edit_counts, to_lengths = calculate_edit_distances(stats_annotations, auto_annotations, align_directory='alignments/rq1.1chars')\n",
    "\n",
    "# print('Edit distances:     ', \"\\t\".join([f\"{k}:{v}\" for k, v in edit_distances.items()]))\n",
    "# print('Total characters:   ', \"\\t\".join([f\"{k}:{v}\" for k, v in to_lengths.items()]))\n",
    "# print('Edits per character:', \"\\t\".join([f\"{k}:{v / to_lengths[k]:.2f}\" for k, v in edit_distances.items()]))\n",
    "# print('Edits per character (macro avg across utts):', \"\\t\".join([f\"{k}:{v / macro_avg_count[k]:.2f}\" for k, v in macro_avg_edit_distance.items()]))\n",
    "\n",
    "# #columns = ['whisper', 'mixed', 'scratch']\n",
    "# columns = ['whisper', 'mixed']\n",
    "# values = [to_lengths[c] for c in columns]\n",
    "# relativ = [edit_distances[c] / to_lengths[c] for c in columns]\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.bar(columns, relativ)\n",
    "# ax.set_ylabel('Edit distance per character')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison over all recordings: annotation time per recording, real-time factor, standardized RTF, character edit distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "\n",
    "os.makedirs('tables', exist_ok=True)\n",
    "\n",
    "annotdurations = defaultdict(list)\n",
    "rtfs = defaultdict(list)\n",
    "normed_rtfs = defaultdict(list)\n",
    "#edits = defaultdict(lambda: 0)\n",
    "#words = defaultdict(lambda: 0)\n",
    "\n",
    "rtfs_per_user_means = {user: np.mean(times) for user, times in rtf_per_user.items()}\n",
    "rtfs_per_user_stds = {user: np.std(times) for user, times in rtf_per_user.items()}\n",
    "\n",
    "for annot_name in stats_annot_durations.keys():\n",
    "    for t in PREFERENCE_ORDER:\n",
    "        if t in annot_name:\n",
    "            break\n",
    "    #if -1 < stats_annot_durations[annot_name] < 10000: #TODO: this is\n",
    "    \n",
    "    annotdurations[t].append(stats_annot_durations[annot_name])\n",
    "    annotdurations['all'].append(stats_annot_durations[annot_name])\n",
    "    \n",
    "    rtf = stats_annot_durations[annot_name] / annotation2duration[annot_name]\n",
    "    rtfs[t].append(rtf)\n",
    "    rtfs['all'].append(rtf)\n",
    "    \n",
    "    user = annot_name.split('-')[1]\n",
    "    normed_rtf = (rtf - rtfs_per_user_means[user]) / rtfs_per_user_stds[user]\n",
    "    normed_rtfs[t].append(normed_rtf)\n",
    "    normed_rtfs['all'].append(normed_rtf)\n",
    "    #edits[t] += annot2edit_distances[annot_name]\n",
    "    #words[t] += annot2numwords[annot_name]\n",
    "\n",
    "row_names = [\"WhisperX\", \"Mixed\", \"From scratch\", \"All\"]\n",
    "\n",
    "table_dict = {\"method\": [], \"annottime_mean\": [], \"annottime_std\": [], \"rtf_mean\": [], \"rtf_std\": [], \"normed_rtf_mean\": [], \"normed_rtf_std\": []}\n",
    "for speaker_type in SPEAKER_TYPES + ['all']:\n",
    "    table_dict[\"ced_mean_\" + speaker_type] = []\n",
    "    table_dict[\"ced_std_\" + speaker_type] = []\n",
    "for i, t in enumerate(PREFERENCE_ORDER + ['all']):\n",
    "    table_dict[\"method\"].append(row_names[i])\n",
    "    table_dict[\"annottime_mean\"].append(np.mean(annotdurations[t]))\n",
    "    table_dict[\"annottime_std\"].append(np.std(annotdurations[t]))\n",
    "    table_dict[\"rtf_mean\"].append(np.mean(rtfs[t]))\n",
    "    table_dict[\"rtf_std\"].append(np.std(rtfs[t]))\n",
    "    table_dict[\"normed_rtf_mean\"].append(np.mean(normed_rtfs[t]))\n",
    "    table_dict[\"normed_rtf_std\"].append(np.std(normed_rtfs[t]))\n",
    "    for speaker_type in SPEAKER_TYPES + ['all']:\n",
    "        table_dict[\"ced_mean_\" + speaker_type].append(np.mean(ceds[speaker_type][t]))\n",
    "        table_dict[\"ced_std_\" + speaker_type].append(np.std(ceds[speaker_type][t]))\n",
    "\n",
    "table = pd.DataFrame(table_dict)\n",
    "headers = [\n",
    "    [\"\\multirow{2}{*}{Method}\", \"\\multirow{2}{*}{\\makecell{Annot. time \\\\\\\\ per recording (s)}}\", \"\\multirow{2}{*}{RTF}\", \"\\multirow{2}{*}{\\makecell{Standardized \\\\\\\\ RTF}}\", \"\\multicolumn{3}{c}{CER (\\%)}\"],\n",
    "    [\"\", \"\", \"\", \"\", \"Examiners\", \"Candidates\", \"All\"]\n",
    "]\n",
    "\n",
    "with open('tables/annot_times.tex', 'w') as f:\n",
    "    f.write(r\"\\begin{tabular}{l ccc ccc}\" + \"\\n\")\n",
    "    f.write(r\"\\toprule\" + \"\\n\")\n",
    "    for i, header in enumerate(headers):\n",
    "        f.write(\" & \".join(header) + r\" \\\\\" + \"\\n\")\n",
    "        if i == 0:\n",
    "            f.write(r\"\\cmidrule(l){5-7}\" + \"\\n\")\n",
    "    f.write(r\"\\midrule\" + \"\\n\")\n",
    "    for i in range(len(PREFERENCE_ORDER)):\n",
    "        cells = [\n",
    "            table_dict['method'][i],\n",
    "            f\"{table_dict['annottime_mean'][i]:.2f} $\\pm$ {table_dict['annottime_std'][i]:.2f}\",\n",
    "            f\"{table_dict['rtf_mean'][i]:.2f} $\\pm$ {table_dict['rtf_std'][i]:.2f}\",\n",
    "            f\"{table_dict['normed_rtf_mean'][i]:.2f} $\\pm$ {table_dict['normed_rtf_std'][i]:.2f}\",\n",
    "            f\"{table_dict['ced_mean_exam'][i]*100:.2f} $\\pm$ {table_dict['ced_std_exam'][i]*100:.2f}\",\n",
    "            f\"{table_dict['ced_mean_cand'][i]*100:.2f} $\\pm$ {table_dict['ced_std_cand'][i]*100:.2f}\",\n",
    "            f\"{table_dict['ced_mean_all'][i]*100:.2f} $\\pm$ {table_dict['ced_std_all'][i]*100:.2f}\"\n",
    "        ]\n",
    "        f.write(\" & \".join(cells) + r\" \\\\\" + \"\\n\")\n",
    "    f.write(r\"\\midrule\" + \"\\n\")\n",
    "    cells = [\n",
    "        table_dict['method'][-1],\n",
    "        f\"{table_dict['annottime_mean'][-1]:.2f} $\\pm$ {table_dict['annottime_std'][-1]:.2f}\",\n",
    "        f\"{table_dict['rtf_mean'][-1]:.2f} $\\pm$ {table_dict['rtf_std'][-1]:.2f}\",\n",
    "        f\"{abs(table_dict['normed_rtf_mean'][-1]):.2f} $\\pm$ {table_dict['normed_rtf_std'][-1]:.2f}\",\n",
    "        f\"{table_dict['ced_mean_exam'][-1]*100:.2f} $\\pm$ {table_dict['ced_std_exam'][-1]*100:.2f}\",\n",
    "        f\"{table_dict['ced_mean_cand'][-1]*100:.2f} $\\pm$ {table_dict['ced_std_cand'][-1]*100:.2f}\",\n",
    "        f\"{table_dict['ced_mean_all'][-1]*100:.2f} $\\pm$ {table_dict['ced_std_all'][-1]*100:.2f}\"\n",
    "    ]\n",
    "    f.write(\" & \".join(cells) + r\" \\\\\" + \"\\n\")\n",
    "    f.write(r\"\\bottomrule\" + \"\\n\")\n",
    "    f.write(r\"\\end{tabular}\" + \"\\n\")\n",
    "\n",
    "#df = pd.DataFrame(table_dict)\n",
    "#df.to_latex('tables/annot_times.tex', header=headers, float_format=\"%.2f\", index=False)\n",
    "\n",
    "print(table)\n",
    "\n",
    "#print('Total times: ', durations)\n",
    "#print('Edit distances:', edits)\n",
    "#print('Annot to recording ratio:', [(k, annotdurations[k] / v) for k, v in durations.items()])\n",
    "#print('Annot time per word:', [(k, annotdurations[k] / v) for k, v in words.items()])\n",
    "#print('Annot duration per edit:', [(k, annotdurations[k] / v) for k, v in edits.items()])\n",
    "#print('Edit distance per word:', [(k, v / words[k]) for k, v in edits.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ2: Does post-editing enhance transcripts’ consistency? Inter-annotator agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are interested only in recordings with multiple transcripts\n",
    "stats_recordings = {recname: annots for recname, annots in before_review_recordings.items() if len(annots) > 1 and \"npi\" not in recname}\n",
    "print(len(stats_recordings.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uttaligner = uttalign.BioAligner()\n",
    "\n",
    "table_dict = {\"annotator1\": [], \"type1\": [], \"annotator2\": [], \"type2\": [], \"iaa12\": [], \"iaa21\": [], \"iaa\": []}\n",
    "\n",
    "for recname, annots in stats_recordings.items():\n",
    "    # iterate over all pairs of transcripts\n",
    "    for i, (annot1_name, annot1) in enumerate(annots.items()):\n",
    "        for annot1_type in PREFERENCE_ORDER:\n",
    "            if annot1_type in annot1_name:\n",
    "                break\n",
    "        for user1 in ANNOTATOR_NAMES:\n",
    "            if user1 in annot1_name:\n",
    "                break\n",
    "        for j, (annot2_name, annot2) in enumerate(annots.items()):\n",
    "            if i >= j:\n",
    "                continue\n",
    "            for annot2_type in PREFERENCE_ORDER:\n",
    "                if annot2_type in annot2_name:\n",
    "                    break\n",
    "            for user2 in ANNOTATOR_NAMES:\n",
    "                if user2 in annot2_name:\n",
    "                    break\n",
    "            # align the transcripts. It should be both_manual=True here, but the results are a bit weird and hard to explain. It needs to be investigated.\n",
    "            # alignment = uttaligner.align_utterances(annot1, annot2, both_manual=True)\n",
    "            alignment = uttaligner.align_utterances(annot1, annot2, both_manual=True)\n",
    "            aligned_texts = [(uttalign.extract_text(i, annot1), uttalign.extract_text(j, annot2)) for i, j in alignment]\n",
    "            aligned_speakers = [(uttalign.extract_speaker(i, annot1), uttalign.extract_speaker(j, annot2)) for i, j in alignment]\n",
    "\n",
    "            edit_count = 0\n",
    "            length1 = 0\n",
    "            length2 = 0\n",
    "\n",
    "            for (text1, text2), (speaker1, speaker2) in zip(aligned_texts, aligned_speakers):\n",
    "                l1 = len(utils.normalize_text(text1, char_level=True))\n",
    "                l2 = len(utils.normalize_text(text2, char_level=True))\n",
    "                agree, disagree, word_alignment = uttalign.align_texts(text1, text2, char_level=True)\n",
    "                edit_count += disagree\n",
    "                length1 += l1\n",
    "                length2 += l2\n",
    "\n",
    "            iaa12 = 1 - (edit_count / length1) if length1 > 0 else 0\n",
    "            iaa21 = 1 - (edit_count / length2) if length2 > 0 else 0\n",
    "            iaa = (iaa12 + iaa21) / 2\n",
    "\n",
    "            table_dict[\"annotator1\"].append(user1)\n",
    "            table_dict[\"type1\"].append(annot1_type)\n",
    "            table_dict[\"annotator2\"].append(user2)\n",
    "            table_dict[\"type2\"].append(annot2_type)\n",
    "            table_dict[\"iaa12\"].append(iaa12)\n",
    "            table_dict[\"iaa21\"].append(iaa21)\n",
    "            table_dict[\"iaa\"].append(iaa)\n",
    "\n",
    "df = pd.DataFrame(table_dict)\n",
    "print(df)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean([row['iaa'] for i, row in df.iterrows()]))\n",
    "print(np.std([row['iaa'] for i, row in df.iterrows()]))\n",
    "\n",
    "# calculate IAAs across types\n",
    "iaa_means_per_type = {t1: {} for t1 in PREFERENCE_ORDER}\n",
    "iaa_stds_per_type = {t1: {} for t1 in PREFERENCE_ORDER}\n",
    "iaa_counts_per_type = {t1: {} for t1 in PREFERENCE_ORDER}\n",
    "for t1 in PREFERENCE_ORDER:\n",
    "    for t2 in PREFERENCE_ORDER:\n",
    "        iaas = [row['iaa']*100 for i, row in df.iterrows() \n",
    "                                         if (row['type1'] == t1 and row['type2'] == t2) or\n",
    "                                            (row['type1'] == t2 and row['type2'] == t1)]\n",
    "        iaa_means_per_type[t1][t2] = np.mean(iaas)\n",
    "        iaa_means_per_type[t2][t1] = iaa_means_per_type[t1][t2]\n",
    "        iaa_stds_per_type[t1][t2] = np.std(iaas)\n",
    "        iaa_stds_per_type[t2][t1] = iaa_stds_per_type[t1][t2]\n",
    "        iaa_counts_per_type[t1][t2] = len(iaas)\n",
    "        iaa_counts_per_type[t2][t1] = iaa_counts_per_type[t1][t2]\n",
    "\n",
    "# plot confusion matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "values = [[iaa_means_per_type[r][c] for c in PREFERENCE_ORDER] for r in PREFERENCE_ORDER]\n",
    "labels = [[f\"{iaa_means_per_type[r][c]:.2f}±{iaa_stds_per_type[r][c]:.2f}\\n   ({iaa_counts_per_type[r][c]})\" for c in PREFERENCE_ORDER] for r in PREFERENCE_ORDER]\n",
    "tick_labels = [\"WhisperX\", \"Mixed\", \"From scratch\"]\n",
    "mask = np.triu(np.ones_like(values, dtype=bool), k=1)\n",
    "sns.heatmap(values, annot=labels, ax=ax, xticklabels=tick_labels, yticklabels=tick_labels, fmt='', linecolor='white', linewidths=1, cmap='coolwarm', mask=mask)\n",
    "ax.set_xlabel('Annotation method')\n",
    "ax.set_ylabel('Annotation method')\n",
    "ax.grid(False)\n",
    "fig.set_size_inches(9, 4)\n",
    "fig.savefig('figures/iaa.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ4: Is the human post-edited transcription biased towards the ASR system it was based on?\n",
    "## measure WER or other ASR metrics on outputs of multiple ASR systems while varying the reference transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_recordings = before_review_recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import uttalign\n",
    "import utils\n",
    "\n",
    "def parse_annot_type(filename):\n",
    "    return filename.split('from_')[-1].split('.')[0]\n",
    "\n",
    "#aligner = uttalign.MatrixAligner(threshold_overlap=0.1)\n",
    "aligner = uttalign.BioAligner()\n",
    "\n",
    "wers = {}\n",
    "for reference_asr, version in auto_recordings_versions.items():\n",
    "    agree, disagree = defaultdict(lambda: 0), defaultdict(lambda: 0)\n",
    "    for recname, asr_transcript in version.items():\n",
    "        recname = recname.replace('.xml', '.mp3')\n",
    "        # asr_transcript = sort_by_time(asr_transcript)\n",
    "        \n",
    "        if recname not in stats_recordings:\n",
    "            continue\n",
    "            \n",
    "        for annot_name, manual_transcript in stats_recordings[recname].items():\n",
    "            annot_type = parse_annot_type(annot_name)\n",
    "\n",
    "            utt_alignment = aligner.align_utterances(manual_transcript, asr_transcript)\n",
    "\n",
    "            aligned_texts = [(uttalign.extract_text(i, manual_transcript), uttalign.extract_text(j, asr_transcript)) for i, j in utt_alignment]\n",
    "            for manual_text, asr_text in aligned_texts:\n",
    "                a, d, _ = uttalign.align_texts(asr_text, manual_text)\n",
    "                agree[annot_type] += a\n",
    "                disagree[annot_type] += d\n",
    "\n",
    "    print(f\"ASR version {reference_asr}:\")\n",
    "    for annot_type in agree.keys():\n",
    "        print(f\"  {annot_type}: {agree[annot_type]} / {disagree[annot_type]}; WER: {disagree[annot_type] / (agree[annot_type] + disagree[annot_type])}\")\n",
    "        wers[f'{reference_asr}_{annot_type}'] = disagree[annot_type] / (agree[annot_type] + disagree[annot_type])\n",
    "\n",
    "# Matrix:\n",
    "# ASR version mms-1b-all:\n",
    "#   whisperX: 6383 / 6843; WER: 0.5173899894147891\n",
    "#   mixed: 9941 / 12326; WER: 0.553554587506175\n",
    "#   scratch: 1472 / 2997; WER: 0.6706198254643096\n",
    "# ASR version mms-1b-fl102:\n",
    "#   mixed: 12038 / 10467; WER: 0.4650966451899578\n",
    "#   scratch: 1836 / 2832; WER: 0.6066838046272494\n",
    "#   whisperX: 7466 / 5853; WER: 0.4394474059614085\n",
    "# ASR version whisperX-large-v2:\n",
    "#   mixed: 16309 / 6317; WER: 0.2791920799080704\n",
    "#   whisperX: 10429 / 2948; WER: 0.22037826119458773\n",
    "#   scratch: 2596 / 2387; WER: 0.47902869757174393\n",
    "# ASR version whisper-large-v3:\n",
    "#   mixed: 14842 / 8707; WER: 0.3697396917066542\n",
    "#   whisperX: 9290 / 4640; WER: 0.3330940416367552\n",
    "#   scratch: 2308 / 2668; WER: 0.5361736334405145\n",
    "\n",
    "# Bio:\n",
    "# ASR version mms-1b-all:\n",
    "#   whisperX: 6394 / 6810; WER: 0.5157528021811573\n",
    "#   mixed: 9959 / 12305; WER: 0.5526859504132231\n",
    "#   scratch: 1496 / 2958; WER: 0.6641221374045801\n",
    "# ASR version mms-1b-fl102:\n",
    "#   mixed: 12093 / 10402; WER: 0.4624138697488331\n",
    "#   scratch: 1856 / 2793; WER: 0.600774360077436\n",
    "#   whisperX: 7509 / 5783; WER: 0.43507372855853144\n",
    "# ASR version whisperX-large-v2:\n",
    "#   mixed: 16432 / 6118; WER: 0.2713082039911308\n",
    "#   whisperX: 10551 / 2726; WER: 0.20531746629509678\n",
    "#   scratch: 2639 / 2292; WER: 0.46481443926181304\n",
    "# ASR version whisper-large-v3:\n",
    "#   mixed: 14907 / 8619; WER: 0.3663606222902321\n",
    "#   whisperX: 9372 / 4499; WER: 0.32434575733544807\n",
    "#   scratch: 2361 / 2566; WER: 0.5208037345240512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a heatmap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(wers)\n",
    "fig, ax = plt.subplots()\n",
    "columns = set(c.split('_')[1] for c in wers.keys())\n",
    "rows = set(c.split('_')[0] for c in wers.keys())\n",
    "sns.heatmap([[wers.get(f'{r}_{c}', -1) for c in columns] for r in rows], annot=True, ax=ax, xticklabels=columns, yticklabels=rows)\n",
    "ax.set_xlabel('Before annotation transcript')\n",
    "ax.set_ylabel('ASR')\n",
    "ax.set_title('Word error rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto to Manual Utterances alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import uttalign\n",
    "\n",
    "os.makedirs('alignment_test', exist_ok=True)\n",
    "\n",
    "matrix_aligner = uttalign.MatrixAligner(threshold_overlap=0.05)\n",
    "bio_aligner = uttalign.BioAligner()\n",
    "\n",
    "for annot_file, manual_annotation in manual_annotations.items():\n",
    "    if annot_file != \"A2ML_221205_15-ZM-from_mixed.xml\":\n",
    "        continue\n",
    "    print(f\"Processing {annot_file}\")\n",
    "    auto_annotation = auto_annotations[annot_file]\n",
    "    if not len(auto_annotation):\n",
    "        continue\n",
    "    matrix_alignment = matrix_aligner.align_utterances(manual_annotation, auto_annotation)\n",
    "    with open(f'alignment_test/{annot_file}_matrix.txt', 'w') as f:\n",
    "        for a, b in matrix_alignment:\n",
    "            a_text = uttalign.extract_text(a, manual_annotation)\n",
    "            b_text = uttalign.extract_text(b, auto_annotation)\n",
    "            print(f\"MANUAL: {a_text}\", file=f)\n",
    "            print(f\"AUTO: {b_text}\", file=f)\n",
    "            print(file=f)\n",
    "\n",
    "    bio_alignment = bio_aligner.align_utterances(manual_annotation, auto_annotation)\n",
    "    with open(f'alignment_test/{annot_file}_bio.txt', 'w') as f:\n",
    "        for a, b in bio_alignment:\n",
    "            a_text = uttalign.extract_text(a, manual_annotation)\n",
    "            b_text = uttalign.extract_text(b, auto_annotation)\n",
    "            print(f\"MANUAL: {a_text}\", file=f)\n",
    "            print(f\"AUTO: {b_text}\", file=f)\n",
    "            print(file=f)\n",
    "\n",
    "\"\"\" with open('alignments.txt', 'w') as f:\n",
    "    for reference_asr, version in auto_recordings_versions.items():\n",
    "        for recname, asr_transcript in version.items():\n",
    "            recname = recname.replace('.xml', '.mp3')\n",
    "            \n",
    "            if recname not in manual_recordings:\n",
    "                continue\n",
    "                \n",
    "            f.write(f\"ASR version {reference_asr} for {recname}:\\n\")\n",
    "            for annot_name, manual_transcript in manual_recordings[recname].items():\n",
    "\n",
    "                alignment = align_utterances(manual_transcript,asr_transcript, )\n",
    "                for idx, (a, b) in enumerate(zip(alignment[0][0], alignment[0][1])):\n",
    "                    if b is not None:\n",
    "                        al = []\n",
    "                        if a is None:\n",
    "                            a = find_best_alignment_for_auto(alignment[0], idx)\n",
    "                        if a is not None:\n",
    "                            al.extend(a['original_utterances'])\n",
    "                        # print(b['original_utterances'])\n",
    "                        f.write(f\"{b['original_utterances']} -> {al}\\n\")\n",
    "        #         break\n",
    "        #     break\n",
    "        # break\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    " - Using WhisperX segmentation biases the number of utterances (e.g., 07-npi-test-B1.mp3 with 161 and 273)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: how to compute the total duration?\n",
    "total_duration = sum(list(recording.values())[0][-1]['end'] - list(recording.values())[0][0]['start'] for recording in recordings.values())\n",
    "print(f\"Total recordings:  {len(recordings)}\")\n",
    "print(f\"Total duration:    {total_duration:.02f} seconds ({total_duration/60:.02f} minutes)\")\n",
    "\n",
    "# TODO: should we take the max or select some specific version?\n",
    "total_utterances = sum(max(len(utterances) for utterances in transcripts.values()) for transcripts in recordings.values())\n",
    "print(f\"Total utterances:  {total_utterances}\")\n",
    "\n",
    "total_words = sum(\n",
    "    max(\n",
    "        sum(\n",
    "            len(utterance['text'].split())\n",
    "            for utterance in transcript\n",
    "        )\n",
    "        for transcript in recording.values()\n",
    "    )\n",
    " for recording in recordings.values())\n",
    "print(f\"Total words:       {total_words}\")\n",
    "\n",
    "\n",
    "num_versions_histogram = {}\n",
    "for recording, transcripts in recordings.items():\n",
    "    num_versions = len(transcripts)\n",
    "    num_versions_histogram[num_versions] = num_versions_histogram.get(num_versions, 0) + 1\n",
    "print(f\"Version histogram: {num_versions_histogram}\")\n",
    "\n",
    "num_speakers_histogram = {}\n",
    "for recording, transcripts in recordings.items():\n",
    "    num_speakers = max(len(set([u['speaker'] for u in utterances])) for utterances in transcripts.values())\n",
    "    num_speakers_histogram[num_speakers] = num_speakers_histogram.get(num_speakers, 0) + 1\n",
    "print(f\"Speaker histogram: {num_speakers_histogram}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Students Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "total_student_duration = sum(\n",
    "    max(\n",
    "        sum(\n",
    "            utterance['end'] - utterance['start']\n",
    "            for utterance in transcript\n",
    "            if utterance['speaker'] != 't'\n",
    "        )\n",
    "        for transcript in recording.values()\n",
    "    )\n",
    " for recording in recordings.values())\n",
    "print(f\"Total student duration:    {total_student_duration:.02f} seconds ({total_student_duration/60:.02f} minutes); {total_student_duration/total_duration*100:.02f}% of total duration\")\n",
    "\n",
    "total_student_utterances = sum(\n",
    "    max(\n",
    "        sum(\n",
    "            utterance['speaker'] != 't'\n",
    "            for utterance in transcript\n",
    "        )\n",
    "        for transcript in recording.values()\n",
    "    )\n",
    " for recording in recordings.values())\n",
    "print(f\"Total student utterances:  {total_student_utterances} ({total_student_utterances/total_utterances*100:.02f}% of total utterances)\")\n",
    "\n",
    "total_student_words = sum(\n",
    "    max(\n",
    "        sum(\n",
    "            len(utterance['text'].split())\n",
    "            for utterance in transcript\n",
    "            if utterance['speaker'] != 't'\n",
    "        )\n",
    "        for transcript in recording.values()\n",
    "    )\n",
    " for recording in recordings.values())\n",
    "print(f\"Total student words:       {total_student_words} ({total_student_words/total_words*100:.02f}% of total words)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for level in ['A1', 'A2', 'B1', 'B2', 'C1']:\n",
    "    relevant = [transcript_version_select(r) for n, r in recordings.items() if level in n]\n",
    "    \n",
    "    duration = sum(transcript[-1]['end'] - transcript[0]['start'] for transcript in relevant)\n",
    "    print(f'{level}: {len(relevant)} recordings; {duration:.02f} seconds ({duration/60:.02f} minutes)')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inter-annotator Agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obsolete code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from Bio.pairwise2 import format_alignment\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import uttalign\n",
    "import utils\n",
    "\n",
    "os.makedirs('alignments/rq1.1', exist_ok=True)\n",
    "\n",
    "#annot2texts = {k: concat_texts_by_speaker(v) for k, v in before_review_annotations.items()}\n",
    "final_auto_annot_pairs = {annot_name:(before_review_annotations[annot_name], auto_annotations[annot_name]) for annot_name in before_review_annotations}\n",
    "\n",
    "annot2edit_distances = defaultdict(lambda: 0)\n",
    "annot2numwords = defaultdict(lambda: 0)\n",
    "edit_counts = defaultdict(lambda: 0)\n",
    "to_lengths = defaultdict(lambda: 0)\n",
    "\n",
    "#aligner = uttalign.BioAligner()\n",
    "aligner = uttalign.MatrixAligner(threshold_overlap=0.1)\n",
    "\n",
    "for annot_name, (to_annot, from_annot) in final_auto_annot_pairs.items():\n",
    "    for transcript_type in PREFERENCE_ORDER:\n",
    "        if transcript_type in annot_name:\n",
    "            break\n",
    "\n",
    "    utt_alignment = aligner.align_utterances(to_annot, from_annot)\n",
    "\n",
    "    aligned_texts = [(uttalign.extract_text(i, to_annot), uttalign.extract_text(j, from_annot)) for i, j in utt_alignment]\n",
    "    aligned_speakers = [(uttalign.extract_speaker(i, to_annot), uttalign.extract_speaker(j, from_annot)) for i, j in utt_alignment]\n",
    "\n",
    "    for (to_text, from_text), (to_speaker, from_speaker) in zip(aligned_texts, aligned_speakers):\n",
    "        speaker_type = to_speaker\n",
    "        if to_speaker is not None:\n",
    "            for speaker_type in SPEAKER_TYPES:\n",
    "                if speaker_type.lower() in to_speaker.lower():\n",
    "                    break\n",
    "        l = len(utils.normalize_text(to_text))\n",
    "        #print(before_review_text, auto_text)\n",
    "        agree, disagree, word_alignment = uttalign.align_texts(to_text, from_text)\n",
    "        # if l and disagree / l > 0.9:\n",
    "        #     print(f\"{annot_name} {before_review_speaker} {transcript_type} {agree} {disagree}\")\n",
    "        with open(f'alignments/rq1.1/{annot_name}_{to_speaker}.txt', 'w') as f:\n",
    "            if word_alignment is not None:\n",
    "                f.write(format_alignment(*word_alignment))\n",
    "        if speaker_type:\n",
    "            edit_counts[f\"{transcript_type}:{speaker_type}\"] += disagree\n",
    "            to_lengths[f\"{transcript_type}:{speaker_type}\"] += l\n",
    "        edit_counts[transcript_type] += disagree\n",
    "        to_lengths[transcript_type] += l\n",
    "        annot2edit_distances[annot_name] += disagree\n",
    "        annot2numwords[annot_name] += l\n",
    "\n",
    "print('Edit distances:     ', \"\\t\".join([f\"{k}:{v}\" for k, v in edit_counts.items()]))\n",
    "print('Total characters:   ', \"\\t\".join([f\"{k}:{v}\" for k, v in to_lengths.items()]))\n",
    "print('Edits per character:', \"\\t\".join([f\"{k}:{v / to_lengths[k]:.2f}\" for k, v in edit_counts.items()]))\n",
    "\n",
    "columns = ['whisper', 'mixed']\n",
    "values = [to_lengths[c] for c in columns]\n",
    "relativ = [edit_counts[c] / to_lengths[c] for c in columns]\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(columns, relativ)\n",
    "ax.set_ylabel('Edit distance per word')\n",
    "\n",
    "# Bio\n",
    "# Edit distances:      whisper:exam:1112\twhisper:2721\twhisper:cand:1602\tmixed:exam:3549\tmixed:9251\tmixed:cand:5698\tscratch:cand:3413\tscratch:7237\tscratch:exam:3824\n",
    "# Total characters:    whisper:exam:7676\twhisper:12991\twhisper:cand:5315\tmixed:exam:13174\tmixed:23351\tmixed:cand:10177\tscratch:cand:3413\tscratch:7237\tscratch:exam:3824\n",
    "# Edits per character: whisper:exam:0.14\twhisper:0.21\twhisper:cand:0.30\tmixed:exam:0.27\tmixed:0.40\tmixed:cand:0.56\tscratch:cand:1.00\tscratch:1.00\tscratch:exam:1.00\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison over the recordings that were both manually transcribed and manually post-edited. Total duration. Total edit distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_annot_durations = before_review_annot_durations\n",
    "\n",
    "durations = defaultdict(lambda: 0)\n",
    "annotdurations = defaultdict(lambda: 0)\n",
    "edits = defaultdict(lambda: 0)\n",
    "words = defaultdict(lambda: 0)\n",
    "\n",
    "for annot_name in before_review_annotations.keys():\n",
    "    for t in PREFERENCE_ORDER:\n",
    "        if t in annot_name:\n",
    "            break\n",
    "    #if -1 < stats_annot_durations[annot_name] < 10000: #TODO: this is \n",
    "    durations[t] += annotation2duration[annot_name]\n",
    "    annotdurations[t] += stats_annot_durations[annot_name]\n",
    "    edits[t] += annot2edit_distances[annot_name]\n",
    "    words[t] += annot2numwords[annot_name]\n",
    "\n",
    "\n",
    "print('Total times: ', durations)\n",
    "print('Edit distances:', edits)\n",
    "print('Annot to recording ratio:', [(k, annotdurations[k] / v) for k, v in durations.items()])\n",
    "print('Annot time per word:', [(k, annotdurations[k] / v) for k, v in words.items()])\n",
    "print('Annot duration per edit:', [(k, annotdurations[k] / v) for k, v in edits.items()])\n",
    "print('Edit distance per word:', [(k, v / words[k]) for k, v in edits.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_recordings = [k for k, v in before_review_recordings.items() if any(['scratch' in a for a in v.keys()]) and len(v) > 1]\n",
    "\n",
    "total_words = {}\n",
    "total_duration = {}\n",
    "total_edits = {}\n",
    "total_annot_duration = {}\n",
    "\n",
    "for recording in relevant_recordings:\n",
    "    for annot_name in auto_recordings[recording]:\n",
    "        for t in PREFERENCE_ORDER:\n",
    "            if t in annot_name:\n",
    "                break\n",
    "        if t not in annot_name:\n",
    "            continue\n",
    "        if -1 < annotation2annot_duration[annot_name] < 10000: #TODO: this is \n",
    "            total_words[t] = total_words.get(t, 0) + annot2numwords[annot_name]\n",
    "            total_duration[t] = total_duration.get(t, 0) + annotation2duration[annot_name]\n",
    "            total_edits[t] = total_edits.get(t, 0) + annot2edit_distances[annot_name]\n",
    "            total_annot_duration[t] = total_annot_duration.get(t, 0) + annotation2annot_duration[annot_name]\n",
    "\n",
    "print('Relevant recordings:', len(relevant_recordings))\n",
    "print('Total words: ', total_words)\n",
    "print('Total duration:', total_duration)\n",
    "print('Total edits:', total_edits)\n",
    "print('Total annot duration:', total_annot_duration)\n",
    "print('Edits per word:', [(k, total_edits[k] / v) for k, v in total_words.items()])\n",
    "print('Duration per edit:', [(k, v / total_edits[k]) for k, v in total_duration.items()])\n",
    "print('Annot duration per edit:', [(k, v / total_edits[k]) for k, v in total_annot_duration.items()])\n",
    "print('Annot duration per word:', [(k, v / total_words[k]) for k, v in total_annot_duration.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the duration and edit distance correlate?\n",
    "TODO\n",
    "\n",
    "### Is there a significant difference across annotators? Some might prefer one way of transcribing over the other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_annotator(annot_name):\n",
    "    n = annot_name.split('from')[0]\n",
    "    n = n[:-1].split('-')[-1]\n",
    "    assert len(n) > 0   \n",
    "    return n\n",
    "\n",
    "total_words = {}\n",
    "total_duration = {}\n",
    "total_edits = {}\n",
    "total_annot_duration = {}\n",
    "\n",
    "annotators = set([parse_annotator(a) for a in auto_annotations.keys()])\n",
    "\n",
    "\n",
    "\n",
    "for recording in manual_recordings.keys():\n",
    "    for annot_name in manual_recordings[recording]:\n",
    "        for t in PREFERENCE_ORDER:\n",
    "            if t in annot_name:\n",
    "                break\n",
    "        if t not in annot_name:\n",
    "            continue\n",
    "\n",
    "        if -1 < annotation2annot_duration[annot_name] < 10000: #TODO: this is \n",
    "            annotator = parse_annotator(annot_name)\n",
    "            total_words[f'{t}_{annotator}'] = total_words.get(f'{t}_{annotator}', 0) + annot2numwords[annot_name]\n",
    "            total_duration[f'{t}_{annotator}'] = total_duration.get(f'{t}_{annotator}', 0) + annotation2duration[annot_name]\n",
    "            total_edits[f'{t}_{annotator}'] = total_edits.get(f'{t}_{annotator}', 0) + annot2edit_distances[annot_name]\n",
    "            total_annot_duration[f'{t}_{annotator}'] = total_annot_duration.get(f'{t}_{annotator}', 0) + annotation2annot_duration[annot_name]\n",
    "\n",
    "print('Annotators:', annotators)\n",
    "# print('Total words: ', total_words)\n",
    "# print('Total duration:', total_duration)\n",
    "# print('Total edits:', total_edits)\n",
    "# print('Total annot duration:', total_annot_duration)\n",
    "print('Edits per word:', sorted([(k, total_edits[k] / v) for k, v in total_words.items()], key=lambda x: x[0]))\n",
    "print()\n",
    "print('Annot duration per word:', sorted([(k, total_annot_duration[k] / v) for k, v in total_words.items()], key=lambda x: x[0]))\n",
    "print()\n",
    "# print('Duration per edit:', [(k, v / total_edits[k]) for k, v in total_duration.items()])\n",
    "print('Annot duration per edit:', sorted([(k, v / total_edits[k]) for k, v in total_annot_duration.items()], key=lambda x: x[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns = ['whisper', 'mixed', 'scratch']\n",
    "rows = list(annotators)\n",
    "values = {k: total_annot_duration[k] / v for k, v in total_words.items()}\n",
    "# plot a heatmap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "table = [[values.get(f'{c}_{r}', -1) for c in columns] for r in rows]\n",
    "table, rows = zip(*[(t, r) for t, r in zip(table, rows) if any([v > 0 for v in t])])\n",
    "sns.heatmap(table, annot=True, ax=ax, xticklabels=columns, yticklabels=rows)\n",
    "ax.set_xlabel('Before annotation transcript')\n",
    "ax.set_ylabel('Annotator')\n",
    "ax.set_title('Annotaton duration per word')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['whisper', 'mixed', 'scratch']\n",
    "rows = list(annotators)\n",
    "values = {k: total_annot_duration[k] / v for k, v in total_duration.items()}\n",
    "# plot a heatmap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "table = [[values.get(f'{c}_{r}', -1) for c in columns] for r in rows]\n",
    "table, rows = zip(*[(t, r) for t, r in zip(table, rows) if any([v > 0 for v in t])])\n",
    "sns.heatmap(table, annot=True, ax=ax, xticklabels=columns, yticklabels=rows)\n",
    "ax.set_xlabel('Preference')\n",
    "ax.set_ylabel('Annotator')\n",
    "ax.set_title('Annotator duration per recording duration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word-level IAA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from mosestokenizer import MosesTokenizer, MosesPunctuationNormalizer\n",
    "\n",
    "if not os.path.exists('alignments'):\n",
    "    os.makedirs('alignments')\n",
    "\n",
    "punct_normalizer = MosesPunctuationNormalizer('cs')\n",
    "tokenizer = MosesTokenizer('cs')\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = text.lower().replace('\\n', \" \")\n",
    "    text = punct_normalizer(text)\n",
    "    text = tokenizer(text)\n",
    "    return [t for t in text if t.isalnum()]\n",
    "\n",
    "def align_texts_w(text1, text2):\n",
    "    text1 = normalize_text(text1)\n",
    "    text2 = normalize_text(text2)\n",
    "    alignments = pairwise2.align.globalxs(text1, text2, one_alignment_only=True, gap_char=['-'], open=-1, extend=-1)\n",
    "    \n",
    "    agree, disagree = 0, 0\n",
    "    for a, b in zip(alignments[0][0], alignments[0][1]):\n",
    "        if a == b:\n",
    "            agree += 1\n",
    "        else:\n",
    "            disagree += 1\n",
    "    return agree, disagree, alignments[0]\n",
    "\n",
    "def align_texts(text1, text2):\n",
    "    text1 = list(\"\".join(normalize_text(text1)))\n",
    "    text2 = list(\"\".join(normalize_text(text2)))\n",
    "    alignments = pairwise2.align.globalxs(text1, text2, one_alignment_only=True, open=-1, extend=-1, gap_char=['-'])\n",
    "    \n",
    "    agree, disagree = 0, 0\n",
    "    for a, b in zip(alignments[0][0], alignments[0][1]):\n",
    "        if a == b:\n",
    "            agree += 1\n",
    "        else:\n",
    "            disagree += 1\n",
    "    return agree, disagree, alignments[0]\n",
    "\n",
    "\n",
    "relevant = {n: r for n, r in before_review_recordings.items() if len(r) > 1}\n",
    "\n",
    "def get_type(name):\n",
    "    if 'whisper' in name:\n",
    "        return 'whisper'\n",
    "    if 'mixed' in name:\n",
    "        return 'mixed'\n",
    "    if 'scratch' in name:\n",
    "        return 'scratch'\n",
    "    return 'unknown'\n",
    "\n",
    "agreements = defaultdict(lambda: 0)\n",
    "disagreements = defaultdict(lambda: 0)\n",
    "num_recordings = defaultdict(lambda: 0)\n",
    "\n",
    "aligner = uttalign.BioAligner()\n",
    "\n",
    "for name, transcripts in relevant.items():\n",
    "    transcripts = list(transcripts.items())\n",
    "    print(f\"Recording {name}\")\n",
    "    for idx, (annotation_name1, transcript1) in enumerate(transcripts):\n",
    "        for annotation_name2, transcript2 in transcripts[idx+1:]:\n",
    "            alignment = aligner.align_utterances(transcript1, transcript2, both_manual=True)\n",
    "            aligned_texts = [(uttalign.extract_text(i, transcript1), uttalign.extract_text(j, transcript2)) for i, j in alignment]\n",
    "            aligned_speakers = [(uttalign.extract_speaker(i, transcript1), uttalign.extract_speaker(j, transcript2)) for i, j in alignment]\n",
    "\n",
    "            annot_agree, annot_disagree = 0, 0   \n",
    "            for (t1, t2), (s1, s2) in zip(aligned_texts, aligned_speakers):\n",
    "                agree, disagree, _ = uttalign.align_texts(t1, t2)\n",
    "                # if s1 != s2 and len(t1) > 0 and len(t2) > 0:\n",
    "                #     print(f\"Speaker mismatch: {s1} vs {s2}\")\n",
    "                #     print(t1)\n",
    "                #     print('-'*20)\n",
    "                #     print(t2)\n",
    "                #     print(\"=\"*20)\n",
    "                #     print()\n",
    "                t1 = get_type(annotation_name1)\n",
    "                t2 = get_type(annotation_name2)\n",
    "\n",
    "\n",
    "                \n",
    "                \n",
    "                annot_agree += agree\n",
    "                annot_disagree += disagree\n",
    "            if t1 == \"mixed\" and t2 == \"mixed\":\n",
    "                print(f\"MIXED vs. MIXED: {annotation_name1} {annotation_name2}\")\n",
    "              \n",
    "            #print(f\"  {annotation_name1} vs {annotation_name2} :\\t {annot_agree / (annot_agree + annot_disagree)*100:.02f}% agreement\\t {annot_agree} agree, {annot_disagree} disagree\")\n",
    "            if annot_agree / (annot_agree + annot_disagree) < 0.6:  \n",
    "                continue\n",
    "                \n",
    "            agreements[t1, t2] += annot_agree\n",
    "            disagreements[t1, t2] += annot_disagree\n",
    "            agreements[t2, t1] = agreements[t1, t2]\n",
    "            disagreements[t2, t1] = disagreements[t1, t2]\n",
    "            num_recordings[t1, t2] += 1\n",
    "            num_recordings[t2, t1] = num_recordings[t1, t2]\n",
    "\n",
    "            # for speaker in speakers:\n",
    "            #     if speaker not in t1 or speaker not in t2:\n",
    "            #         continue\n",
    "            #     agree, disagree, alignment = align_texts(t1[speaker], t2[speaker])\n",
    "                \n",
    "            #     speaker = 'teacher' if speaker == 't' else 'student ' + speaker\n",
    "            #     print(f\"  {n1} vs {n2} ({speaker}):\\t {agree/(agree+disagree)*100:.02f}% agreement\\t {agree} agree, {disagree} disagree\")\n",
    "            #     if agree / (agree + disagree) < 0.25:\n",
    "            #         continue\n",
    "            #     agreements[get_type(n1), get_type(n2)] = agreements.get((get_type(n1), get_type(n2)), 0) + agree\n",
    "            #     disagreements[get_type(n1), get_type(n2)] = disagreements.get((get_type(n1), get_type(n2)), 0) + disagree\n",
    "                \n",
    "            #     agreements[get_type(n2), get_type(n1)] = agreements[get_type(n1), get_type(n2)]\n",
    "            #     disagreements[get_type(n2), get_type(n1)] = disagreements[get_type(n1), get_type(n2)]\n",
    "\n",
    "            #     with open(f'alignments/{name}_{n1}_{n2}_{speaker}.txt', 'w') as f:\n",
    "            #         f.write(format_alignment(*alignment))\n",
    "            #         f.write(f\"\\n{agree/(agree+disagree)*100:.02f}% agreement\\t {agree} agree, {disagree} disagree\\n\")\n",
    "\n",
    "\n",
    "\n",
    "print(agreements)\n",
    "print(disagreements)\n",
    "print({k: v / (disagreements[k] + v) for k, v in agreements.items()})\n",
    "\n",
    "# plot confusion matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap([[agreements.get((r, c), 0) / (disagreements.get((r, c), 0) + agreements.get((r, c), 1)) for c in ['whisper', 'mixed', 'scratch']] for r in ['whisper', 'mixed', 'scratch']], annot=True, ax=ax, xticklabels=['whisper', 'mixed', 'scratch'], yticklabels=['whisper', 'mixed', 'scratch'])\n",
    "ax.set_xlabel('Reference')\n",
    "ax.set_ylabel('Hypothesis')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "naki",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
